
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 2: Bigram Model &#8212; Mastering Deep Learning: From Neural Networks to Transformers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/ch2';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="References" href="references.html" />
    <link rel="prev" title="Chapter 1: Neural Networks" href="ch1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Mastering Deep Learning: From Neural Networks to Transformers - Home"/>
    <img src="../_static/logo.png" class="logo__image only-dark pst-js-only" alt="Mastering Deep Learning: From Neural Networks to Transformers - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="index.html">Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="ch1.html">Chapter 1: Neural Networks</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 2: Bigram Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/danielsimon4/deep-learning-book" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/ch2.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 2: Bigram Model</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-data-types">2.1 Python Data Types</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-and-explore-a-text-file">2.2 Load and Explore a Text File</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mapping">2.3 Mapping</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#counts-matrix">2.4 Counts Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-matrix">2.5 Probability Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-from-a-probability-distribution">2.6 Sample From a Probability Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#average-negative-log-likelihood">2.7 Average Negative Log Likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-set">2.8 Training Set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding">2.9 One-hot Encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">1.6 Softmax</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">2.12 Loss function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-pass">2.13 Backward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#all-together">2.14 All together</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-2-bigram-model">
<h1>Chapter 2: Bigram Model<a class="headerlink" href="#chapter-2-bigram-model" title="Link to this heading">#</a></h1>
<p>Andrej Karpathy created <a class="reference external" href="https://github.com/karpathy/makemore"><strong>Makemore</strong></a>, an autoregressive <strong>character-level language model</strong> that generates new words based on a given text file input. In his video lecture series, he explains how to build it step by step, starting with a bigram model (covered in this chapter), followed by a multilayer perceptron (Chapter 3), and a WaveNet model (Chapter 4). All models are trained using the <a class="reference external" href="https://github.com/danielsimon4/language-modeling/blob/main/Makemore/names.txt">names dataset</a> from <a class="reference external" href="https://www.ssa.gov/oact/babynames/">ssa.gov</a>, which contains the 32K most common names from 2018.</p>
<br>
<section id="python-data-types">
<span id="id1"></span><h2>2.1 Python Data Types<a class="headerlink" href="#python-data-types" title="Link to this heading">#</a></h2>
<p>In this chapter, I will be using various <strong>built-in data types</strong> in Python. Understanding the differences between them is crucial for efficiently leveraging their unique features and capabilities.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<ul class="simple">
<li><p><strong>Mutable</strong> means that an object can be modified after it is created.</p></li>
<li><p><strong>Ordered</strong> means that the elements in an object have a defined sequence, and they maintain that order when you access or iterate over them.</p></li>
<li><p>All the data types in the table, but the character, are <strong>iterable objects</strong>.</p></li>
</ul>
</div>
<br>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Data Type</strong></p></th>
<th class="head"><p><strong>Definition</strong></p></th>
<th class="head"><p><strong>Example</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>String</p></td>
<td><p>An immutable sequence of characters</p></td>
<td><p>str = ‘apple’</p></td>
</tr>
<tr class="row-odd"><td><p>Character</p></td>
<td><p>A single element of a string</p></td>
<td><p>ch = ‘a’</p></td>
</tr>
<tr class="row-even"><td><p>List</p></td>
<td><p>An ordered and mutable collection of elements</p></td>
<td><p>list = [1, ‘apple’, 3.14, 1]’</p></td>
</tr>
<tr class="row-odd"><td><p>Tuple</p></td>
<td><p>An ordered and immutable collection of elements</p></td>
<td><p>tuple = (1, ‘apple’, 3.14, 1)</p></td>
</tr>
<tr class="row-even"><td><p>Set</p></td>
<td><p>An unordered and mutable collection of unique elements</p></td>
<td><p>set = {1, ‘apple’, 3.14}</p></td>
</tr>
<tr class="row-odd"><td><p>Dictionary</p></td>
<td><p>An unordered and mutable collection of key-value pairs, where each key is unique</p></td>
<td><p>dict = {1: ‘apple’, 2: ‘banana’}</p></td>
</tr>
</tbody>
</table>
</div>
<br>
<br>
</section>
<section id="load-and-explore-a-text-file">
<span id="id2"></span><h2>2.2 Load and Explore a Text File<a class="headerlink" href="#load-and-explore-a-text-file" title="Link to this heading">#</a></h2>
<p>We are going to open the text file <code class="docutils literal notranslate"><span class="pre">names.txt</span></code> and read its contents, storing each line as an element in the list <code class="docutils literal notranslate"><span class="pre">words</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">file.read()</span></code> method reads the entire content of a file and returns it as a single string.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">str.splitlines()</span></code> method splits a string into a list, where each element corresponds to a line, removing the newline characters.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">len(iterable)</span></code> function returns the number of elements in an iterable object.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../datasets/names.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Length of the dataset: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="si">}</span><span class="s1"> words&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Length of the dataset: 32033 words
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print first 10 words</span>
<span class="n">words</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;emma&#39;,
 &#39;olivia&#39;,
 &#39;ava&#39;,
 &#39;isabella&#39;,
 &#39;sophia&#39;,
 &#39;charlotte&#39;,
 &#39;mia&#39;,
 &#39;amelia&#39;,
 &#39;harper&#39;,
 &#39;evelyn&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># find the length of the words</span>
<span class="n">length_words</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Shortest word: </span><span class="si">{</span><span class="nb">min</span><span class="p">(</span><span class="n">length_words</span><span class="p">)</span><span class="si">}</span><span class="s1"> characters&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Longest word: </span><span class="si">{</span><span class="nb">max</span><span class="p">(</span><span class="n">length_words</span><span class="p">)</span><span class="si">}</span><span class="s1"> characters&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shortest word: 2 characters
Longest word: 15 characters
</pre></div>
</div>
</div>
</div>
<br>
</section>
<section id="mapping">
<h2>2.3 Mapping<a class="headerlink" href="#mapping" title="Link to this heading">#</a></h2>
<p>We are going to create two dictionaries, one that <strong>maps (converts)</strong> each character to a unique integer index, and another one that maps the integer indeces back to the corresponding characters.</p>
<br>
<p><strong>Note:</strong></p>
<ul class="simple">
<li><p>The <em>‘’.join(iterable)</em> method concatenates all the strings in the iterable object into a single string. The empty string (‘’) specifies that no characters are inserted between elements during concatenation.</p></li>
<li><p>The <em>set(iterable)</em> function creates a set from an iterable removing any duplicate elements and stores only unique values.</p></li>
<li><p>The <em>list(iterable)</em> function converts an iterable object into a list of its elements.</p></li>
<li><p>The <em>sorted(iterable, key)</em> function sorts the elements of an iterable object based on the criteria provided by the key function.</p></li>
<li><p>The <em>enumerate(iterable, start)</em> function returns an iterator that produces pairs of an index (starting at start) and the corresponding element from the iterable.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># list of unique characters</span>
<span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">))))</span>

<span class="c1"># dicitonary mapping characters to integers</span>
<span class="n">ch_to_int</span> <span class="o">=</span> <span class="p">{</span><span class="n">ch</span><span class="p">:</span><span class="nb">int</span> <span class="k">for</span> <span class="nb">int</span><span class="p">,</span> <span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">)}</span>

<span class="c1"># add the period character (which represents the Start and End characters)</span>
<span class="n">ch_to_int</span><span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># dicitonary mapping integers to characters </span>
<span class="n">int_to_ch</span> <span class="o">=</span> <span class="p">{</span><span class="nb">int</span><span class="p">:</span><span class="n">ch</span> <span class="k">for</span> <span class="n">ch</span><span class="p">,</span> <span class="nb">int</span> <span class="ow">in</span> <span class="n">ch_to_int</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Character to integers:&#39;</span><span class="p">,</span> <span class="n">ch_to_int</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Integers to characters:&#39;</span><span class="p">,</span> <span class="n">int_to_ch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Character to integers: {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 4, &#39;e&#39;: 5, &#39;f&#39;: 6, &#39;g&#39;: 7, &#39;h&#39;: 8, &#39;i&#39;: 9, &#39;j&#39;: 10, &#39;k&#39;: 11, &#39;l&#39;: 12, &#39;m&#39;: 13, &#39;n&#39;: 14, &#39;o&#39;: 15, &#39;p&#39;: 16, &#39;q&#39;: 17, &#39;r&#39;: 18, &#39;s&#39;: 19, &#39;t&#39;: 20, &#39;u&#39;: 21, &#39;v&#39;: 22, &#39;w&#39;: 23, &#39;x&#39;: 24, &#39;y&#39;: 25, &#39;z&#39;: 26, &#39;.&#39;: 0}
Integers to characters: {1: &#39;a&#39;, 2: &#39;b&#39;, 3: &#39;c&#39;, 4: &#39;d&#39;, 5: &#39;e&#39;, 6: &#39;f&#39;, 7: &#39;g&#39;, 8: &#39;h&#39;, 9: &#39;i&#39;, 10: &#39;j&#39;, 11: &#39;k&#39;, 12: &#39;l&#39;, 13: &#39;m&#39;, 14: &#39;n&#39;, 15: &#39;o&#39;, 16: &#39;p&#39;, 17: &#39;q&#39;, 18: &#39;r&#39;, 19: &#39;s&#39;, 20: &#39;t&#39;, 21: &#39;u&#39;, 22: &#39;v&#39;, 23: &#39;w&#39;, 24: &#39;x&#39;, 25: &#39;y&#39;, 26: &#39;z&#39;, 0: &#39;.&#39;}
</pre></div>
</div>
</div>
</div>
<br>
</section>
<section id="counts-matrix">
<h2>2.4 Counts Matrix<a class="headerlink" href="#counts-matrix" title="Link to this heading">#</a></h2>
<p>A bigram is a <strong>pair of consecutive characters</strong> in a string. In a <strong>bigram character level language model</strong> we just work with two characters at a time; looking at the first one we try to predict the second one.</p>
<p>We are going to store the counts of each bigram’s occurrence in the text file in a two-dimensional tensor, commonly known as a <strong>counts matrix</strong>. In the counts matrix <code class="docutils literal notranslate"><span class="pre">N</span></code>, the rows represent the first character of the bigram, and the columns represent the second character.</p>
<br>
<p><strong>Note:</strong></p>
<ul class="simple">
<li><p>The <em>zip(iterable1, iterable2)</em> function pairs corresponding elements from two iterable objects. In this case, <em>zip(chs, chs[1:])</em> pairs each element in the list <code class="docutils literal notranslate"><span class="pre">chs</span></code> with the next element.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import PyToch library</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># create a matrix size (27, 27) filled with zeros</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="mi">27</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>

    <span class="c1"># create a list that contains a start character,</span>
    <span class="c1"># the characters of the word, and an end character</span>
    <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>

    <span class="c1"># create bigrams from the list of characters</span>
    <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>

        <span class="c1"># get the indices of the characters</span>
        <span class="n">ix1</span> <span class="o">=</span> <span class="n">ch_to_int</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
        <span class="n">ix2</span> <span class="o">=</span> <span class="n">ch_to_int</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>

        <span class="c1"># increase the count of that bigram</span>
        <span class="n">N</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<br>
<p>To visualize the matrix <code class="docutils literal notranslate"><span class="pre">N</span></code>, we will use the Python library <a class="reference external" href="https://matplotlib.org/">Matplotlib</a>. Do not worry if you do not fully understand the following code; the key ideas to focus on are:</p>
<ul class="simple">
<li><p>The <strong>first row</strong> shows the counts for <strong>starting characters</strong>.</p></li>
<li><p>The <strong>first column</strong> shows the counts for <strong>ending characters</strong>.</p></li>
<li><p>Each entry in the matrix represents <strong>how often the second character follows the first one</strong>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd"># import matplotlib library</span>
<span class="sd">import matplotlib.pyplot as plt</span>
<span class="sd">%matplotlib inline</span>

<span class="sd"># create figure</span>
<span class="sd">plt.figure(figsize=(16,16))</span>
<span class="sd">plt.imshow(N, cmap=&#39;Blues&#39;)</span>

<span class="sd">for i in range(27):</span>

<span class="sd">    for j in range(27):</span>

<span class="sd">        bigram = int_to_ch[i] + int_to_ch[j]</span>

<span class="sd">        # display bigrams</span>
<span class="sd">        plt.text(j, i, bigram, ha=&quot;center&quot;, va=&quot;bottom&quot;, color=&#39;gray&#39;)</span>

<span class="sd">        # display counts</span>
<span class="sd">        plt.text(j, i, N[i, j].item(), ha=&quot;center&quot;, va=&quot;top&quot;, color=&#39;gray&#39;)</span>

<span class="sd">plt.axis(&#39;off&#39;);</span>

<span class="sd">&quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;\n\n# import matplotlib library\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# create figure\nplt.figure(figsize=(16,16))\nplt.imshow(N, cmap=\&#39;Blues\&#39;)\n\nfor i in range(27):\n\n    for j in range(27):\n\n        bigram = int_to_ch[i] + int_to_ch[j]\n\n        # display bigrams\n        plt.text(j, i, bigram, ha=&quot;center&quot;, va=&quot;bottom&quot;, color=\&#39;gray\&#39;)\n\n        # display counts\n        plt.text(j, i, N[i, j].item(), ha=&quot;center&quot;, va=&quot;top&quot;, color=\&#39;gray\&#39;)\n\nplt.axis(\&#39;off\&#39;);\n\n&#39;
</pre></div>
</div>
</div>
</div>
<br>
</section>
<section id="probability-matrix">
<h2>2.5 Probability Matrix<a class="headerlink" href="#probability-matrix" title="Link to this heading">#</a></h2>
<p>We are going to normalize the counts matrix <code class="docutils literal notranslate"><span class="pre">N</span></code> to create the <strong>probability matrix</strong> <code class="docutils literal notranslate"><span class="pre">P</span></code>. In the matrix <code class="docutils literal notranslate"><span class="pre">P</span></code>, all the rows are <strong>probability distributions</strong> that indicate the probability for the next character given that the previous character is the one corresponding to that row.</p>
<p>Please note that the matrix <code class="docutils literal notranslate"><span class="pre">N</span></code> is incremented by 1 to handle cases where any entry might be zero. This is known as <strong>Laplace smoothing</strong> (or add-one smoothing), and prevents divisions by zero during normalization.</p>
<br>
<p><strong>Note:</strong></p>
<ul class="simple">
<li><p>The <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.float.html"><em>tensor.float()</em></a> method converts the elements of the tensor to float data type (decimal).</p></li>
<li><p>The <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"><em>tensor.sum(1, keepdims=True)</em></a> returns the sum of the elements of the matrix across the rows (horizontally). <em>keepdims=True</em> ensures the dimensions of the output tensor are equal to the input tensor (in this case 2).</p></li>
<li><p><em>P /= sum</em> divides each element of the matrix P by the corresponding row sum. <em>P /=</em> sum is equivalent to <em>P = P / sum</em> but the inplace operation is more efficient since it does not create an additional matrix <code class="docutils literal notranslate"><span class="pre">P</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define matrix P</span>
<span class="n">P</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="c1"># normalize matrix P</span>
<span class="n">P</span> <span class="o">/=</span> <span class="n">P</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<br>
<p>We can index into the matrix <code class="docutils literal notranslate"><span class="pre">P</span></code> to get the row containing the probability distribution for the next character, given that the previous character was the one corresponding to that row.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># probabilites for the bigrams a., aa, ab, ac, ad, ...</span>
<span class="n">P</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.1958, 0.0164, 0.0160, 0.0139, 0.0308, 0.0204, 0.0040, 0.0050, 0.0688,
        0.0487, 0.0052, 0.0168, 0.0746, 0.0482, 0.1604, 0.0019, 0.0024, 0.0018,
        0.0963, 0.0330, 0.0203, 0.0113, 0.0246, 0.0048, 0.0054, 0.0605, 0.0129])
</pre></div>
</div>
</div>
</div>
<br>
</section>
<section id="sample-from-a-probability-distribution">
<h2>2.6 Sample From a Probability Distribution<a class="headerlink" href="#sample-from-a-probability-distribution" title="Link to this heading">#</a></h2>
<p>We are going to use the probability distributions in the matrix <code class="docutils literal notranslate"><span class="pre">P</span></code> to generate 5 character sequences (words) by repeatedly sampling characters. Each sequence starts at index 0 (which corresponds to the start character ‘.’). At each iteration, the <strong>next character is sampled</strong> based on the probability distribution corresponding to the last character sampled. This process continues until index 0 (which also corresponds to the end character ‘.’) is sampled again.</p>
<br>
<p><strong>Note:</strong></p>
<ul class="simple">
<li><p>The <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.item.html"><em>tensor.item()</em></a> method returns the value of the single-element tensor.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># seed the random number generator for reproducibility</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">2147483648</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    
    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># start character</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>

        <span class="c1"># get the probabilty distribution for the next character</span>
        <span class="c1"># that corresponds to the current last character in the sequence</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>

        <span class="c1"># sample from the probability distribution</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="c1"># stop sequence generation when model samples end character</span>
        <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># append nex sampled character into the sequence</span>
        <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">int_to_ch</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>can
ahior
slea
eman
areiakialaveiphali
</pre></div>
</div>
</div>
</div>
<br>
</section>
<section id="average-negative-log-likelihood">
<h2>2.7 Average Negative Log Likelihood<a class="headerlink" href="#average-negative-log-likelihood" title="Link to this heading">#</a></h2>
<p>To evaluate the quality of the generated words, we are going to use the <strong>average negative log likelihood</strong> as our metric.</p>
<p>The <strong>likelihood</strong> is the product of all the individual probabilities of the bigrams chosen by the model when making predictions. Since each probability is between 0 and 1, their product is a very small number. For convenience, we instead use the <strong>log likelihood</strong>, which simply takes the log of the likelihood. Remember that the log of a product is equivalent to the sum of the logs of its factors:</p>
<p><em>log(a · b · c) = log(a) + log(b) + log(c)</em></p>
<p>Thus, the log likelihood can be computed by computing the <strong>sum of all the individual log probabilities</strong>. Please also note that if we pass in a probability of 1 into the log function, we get a log probability of 0. If we pass in a lower probability, the log probability is more negative. Finally, if we pass in a probability of 0, the log probability is -∞.</p>
<div style="width: 300px; margin: 0 auto; text-align: center;">
    <p>f(x)=log(x)</p>
    <img src="https://raw.githubusercontent.com/danielsimon4/language-modeling/refs/heads/main/Images/log-function.png">
</div>
<br>
<p>Hence, when all the individual probabilties are 1 (the best-case scenario), the log likelihood is 0. As the probabilities decrease, the log likelihood becomes increasingly negative. To make this metric easier to interpret, we use the <strong>negative log likelihood (NLL)</strong>, which inverts the scale, and provides a positive metric where values closer to 0 indicate better predictions. Typically, we use the <strong>average negative log likelihood</strong>, which normalizes the NLL by dividing it by the total number of predictions.</p>
<br>
<p>Please note that the average negative log likelihood of a common word like ‘Anna’ is low because the individual probabilities of its bigrams are high.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># individual probabilities list</span>
<span class="n">probs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># list of characters</span>
<span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="s1">&#39;anna&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Bigrams</span><span class="se">\t</span><span class="s1">    Probabilites&#39;</span><span class="p">)</span>

<span class="c1"># create bigrams from the list of characters</span>
<span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>

    <span class="c1"># get the indices of the characters</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">ch_to_int</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">ch_to_int</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>

    <span class="c1"># get the probability of that bigram</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
    
    <span class="c1"># append the probability to the list</span>
    <span class="n">probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">ch1</span><span class="si">}{</span><span class="n">ch2</span><span class="si">}</span><span class="se">\t</span><span class="s1">    </span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># convert list to tensor</span>
<span class="n">probs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Average negative log likelihood: </span><span class="si">{</span><span class="o">-</span><span class="n">probs_tensor</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Bigrams	    Probabilites
.a	    0.1376
an	    0.1604
nn	    0.1039
na	    0.1623
a.	    0.1958

Average negative log likelihood: 1.9054
</pre></div>
</div>
</div>
</div>
<p>However, the average negative log likelihood of a weird word like ‘rwxz’ is high because the individual probabilities of its bigrams are low.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># individual probabilities list</span>
<span class="n">probs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># list of characters</span>
<span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="s1">&#39;rwxz&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Bigrams</span><span class="se">\t</span><span class="s1">    Probabilites&#39;</span><span class="p">)</span>

<span class="c1"># create bigrams from the list of characters</span>
<span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>

    <span class="c1"># get the indices of the characters</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">ch_to_int</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">ch_to_int</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>

    <span class="c1"># get the probability of that bigram</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
    
    <span class="c1"># append the probability to the list</span>
    <span class="n">probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">ch1</span><span class="si">}{</span><span class="n">ch2</span><span class="si">}</span><span class="se">\t</span><span class="s1">    </span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># convert list to tensor</span>
<span class="n">probs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Average negative log likelihood: </span><span class="si">{</span><span class="o">-</span><span class="n">probs_tensor</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Bigrams	    Probabilites
.r	    0.0512
rw	    0.0017
wx	    0.0010
xz	    0.0276
z.	    0.0664

Average negative log likelihood: 4.4995
</pre></div>
</div>
</div>
</div>
<br>
</section>
<section id="training-set">
<h2>2.8 Training Set<a class="headerlink" href="#training-set" title="Link to this heading">#</a></h2>
<p>The elements of the probability matrix <code class="docutils literal notranslate"><span class="pre">P</span></code> can be seen as the <strong>parameters</strong> of an already trained <strong>bigram language model</strong>.</p>
<p>From now on, instead of using the probability distributions in the matrix <code class="docutils literal notranslate"><span class="pre">P</span></code>, we will use a <strong>neural network</strong>. This neural network will <strong>receive a single character</strong> as an input and will <strong>output the probability distribution</strong> for the next character (given the input character).</p>
<p>Our neural network will need to be trained first using a <strong>training set of bigrams</strong> made up of two <strong>one-dimensional tensors</strong>:</p>
<ul class="simple">
<li><p>The input tensor <code class="docutils literal notranslate"><span class="pre">xs</span></code> contains the indeces corresponding to the first character of the bigrams.</p></li>
<li><p>The target tensor <code class="docutils literal notranslate"><span class="pre">ys</span></code> contains the indeces corresponding to the second character of the bigrams.</p></li>
</ul>
<p>To better understand what is happening in the neural network, we are first going to train it with the first word, which contains 5 bigram examples.</p>
<br>
<p><strong>Note:</strong></p>
<ul class="simple">
<li><p>xs.nelement() vs len(xs)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># input and targets lists</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[:</span><span class="mi">1</span><span class="p">]:</span>

    <span class="c1"># list of characters</span>
    <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>

    <span class="c1"># create bigrams from the list of characters</span>
    <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>

        <span class="c1"># get the indices of the characters</span>
        <span class="n">ix1</span> <span class="o">=</span> <span class="n">ch_to_int</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
        <span class="n">ix2</span> <span class="o">=</span> <span class="n">ch_to_int</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>

        <span class="c1"># append the indices to the lists</span>
        <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix1</span><span class="p">)</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix2</span><span class="p">)</span>

<span class="c1"># convert the lists to PyTorch tensors</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Input tensor:&#39;</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Target tensor:&#39;</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of examples:&#39;</span><span class="p">,</span> <span class="n">xs</span><span class="o">.</span><span class="n">nelement</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input tensor: tensor([ 0,  5, 13, 13,  1])
Target tensor: tensor([ 5, 13, 13,  1,  0])
Number of examples: 5
</pre></div>
</div>
</div>
</div>
<br>
</section>
<section id="one-hot-encoding">
<h2>2.9 One-hot Encoding<a class="headerlink" href="#one-hot-encoding" title="Link to this heading">#</a></h2>
<p>The input to a neural network cannot be a tensor of integers, so we will <strong>one-hot encode</strong> the tensor <code class="docutils literal notranslate"><span class="pre">xs</span></code>. This involves converting each integer into a 27-element vector, where all elements are 0 except for a 1 at the position corresponding to the integer’s index.</p>
<br>
<p><strong>Note:</strong></p>
<ul class="simple">
<li><p><em>torch.float()</em> method converts the data type of the one-hot encoded tensor from integer to floating-point. This is often necessary in neural network computations.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">xenc</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># intitialize randomly weight matrix</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="mi">27</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># matrix multiplication</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span> <span class="c1"># (5, 27) = (5, 27) x (27, 27)</span>
<span class="n">logits</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.6360, -0.5165,  0.8938, -1.1803, -0.2712, -1.4676, -0.4764,  0.2647,
         -0.1514, -0.2904, -1.4312, -0.9346,  1.3618,  0.7286,  0.0694, -0.5568,
         -1.3885, -0.9429,  0.6870,  0.3316, -0.9512,  0.0477,  0.4396,  0.3375,
         -0.1883,  0.0425,  1.7159],
        [-0.2973,  0.4501, -1.3273,  1.5327, -0.0991, -0.7760,  0.6792,  0.6085,
         -1.3631, -0.6645,  1.0314,  0.4754, -0.2152,  1.2425, -0.0687, -0.2073,
          0.3700,  0.9314, -0.0936,  1.2581, -0.0597, -0.4705,  0.3833, -0.7627,
          1.0120, -0.7223,  0.3384],
        [-0.8167,  0.6329,  0.1473, -0.4608,  1.2573, -0.1303, -0.4243, -1.4333,
          1.6912, -1.7415,  0.1019,  2.1628,  0.1921, -0.0054,  1.1338, -0.4060,
         -0.5626,  1.1104,  0.5522, -0.1036,  1.7544,  0.5548,  0.2817, -0.0286,
          1.3852, -0.2753,  1.7198],
        [-0.8167,  0.6329,  0.1473, -0.4608,  1.2573, -0.1303, -0.4243, -1.4333,
          1.6912, -1.7415,  0.1019,  2.1628,  0.1921, -0.0054,  1.1338, -0.4060,
         -0.5626,  1.1104,  0.5522, -0.1036,  1.7544,  0.5548,  0.2817, -0.0286,
          1.3852, -0.2753,  1.7198],
        [-0.5411,  1.6627, -0.2231,  2.2653, -0.4916, -0.1598, -0.0657,  1.1353,
          0.5805, -0.5822,  0.3708,  1.3388, -1.3731,  0.2536,  0.3509, -0.1880,
          1.6852, -1.2255, -1.9705,  0.5056,  1.5747,  0.1403, -0.2857, -1.1298,
         -1.4438,  1.6083, -0.3068]], grad_fn=&lt;MmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<br>
</section>
<section id="softmax">
<span id="id3"></span><h2>1.6 Softmax<a class="headerlink" href="#softmax" title="Link to this heading">#</a></h2>
<p>Right now, the neural network is outputing 2 positive and negative numbers for each input example, known as <strong>logits</strong> (or log-counts).</p>
<p>We are going to apply <strong>Softmax</strong>, an activation function often used in the output layer of neural networks to <strong>convert logits into probability distributions</strong>. Softmax first exponentiates the logits to get only positive numbers, known as <strong>counts</strong>, and then normalizes the counts to get <strong>probability distributions</strong>.</p>
<div style="width: 310px; margin: 0 auto;">
    <img src="https://images.contentstack.io/v3/assets/bltac01ee6daa3a1e14/blte5e1674e3883fab3/65ef8ba4039fdd4df8335b7c/img_blog_image1_inline_(2).png?width=1024&disable=upscale&auto=webp">
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tensor.sum(1,</span> <span class="pre">keepdims=True)</span></code> returns the sum of the elements of the matrix across the rows (horizontally). <em>keepdims=True</em> ensures the dimensions of the output tensor are equal to the input tensor (in this case 2). <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum">PyTorch documentation</a>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># softmax</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="c1"># (5, 27)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># (5, 27)</span>
<span class="n">probs</span> <span class="c1"># (5, 27)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.0163, 0.0184, 0.0752, 0.0095, 0.0235, 0.0071, 0.0191, 0.0401, 0.0264,
         0.0230, 0.0074, 0.0121, 0.1201, 0.0638, 0.0330, 0.0176, 0.0077, 0.0120,
         0.0612, 0.0429, 0.0119, 0.0323, 0.0478, 0.0431, 0.0255, 0.0321, 0.1712],
        [0.0185, 0.0391, 0.0066, 0.1154, 0.0226, 0.0115, 0.0491, 0.0458, 0.0064,
         0.0128, 0.0699, 0.0401, 0.0201, 0.0863, 0.0233, 0.0202, 0.0361, 0.0632,
         0.0227, 0.0877, 0.0235, 0.0156, 0.0366, 0.0116, 0.0685, 0.0121, 0.0349],
        [0.0077, 0.0329, 0.0203, 0.0110, 0.0615, 0.0154, 0.0114, 0.0042, 0.0949,
         0.0031, 0.0194, 0.1521, 0.0212, 0.0174, 0.0543, 0.0117, 0.0100, 0.0531,
         0.0304, 0.0158, 0.1011, 0.0305, 0.0232, 0.0170, 0.0699, 0.0133, 0.0976],
        [0.0077, 0.0329, 0.0203, 0.0110, 0.0615, 0.0154, 0.0114, 0.0042, 0.0949,
         0.0031, 0.0194, 0.1521, 0.0212, 0.0174, 0.0543, 0.0117, 0.0100, 0.0531,
         0.0304, 0.0158, 0.1011, 0.0305, 0.0232, 0.0170, 0.0699, 0.0133, 0.0976],
        [0.0108, 0.0982, 0.0149, 0.1794, 0.0114, 0.0159, 0.0174, 0.0579, 0.0333,
         0.0104, 0.0270, 0.0710, 0.0047, 0.0240, 0.0264, 0.0154, 0.1004, 0.0055,
         0.0026, 0.0309, 0.0899, 0.0214, 0.0140, 0.0060, 0.0044, 0.0930, 0.0137]],
       grad_fn=&lt;DivBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<br>
</section>
<section id="loss-function">
<h2>2.12 Loss function<a class="headerlink" href="#loss-function" title="Link to this heading">#</a></h2>
<p>During evaluation, we used the average negative log likelihood to assess the quality of the generated words. During training, we will use it as a <strong>loss function</strong> to <strong>evaluate the setting of the parameters</strong> of the neural network.</p>
<p>To compute the average negative log likelihood, we are now going to use the individual probabilities assigned by the neural network to the actual correct next characters. In this case,the individual probabilites would be:</p>
<p><em>probs[0, 5], probs[1, 13], probs[2, 13], probs[3, 1], and probs[4, 0]</em></p>
<p>We can retrieve these probabilities by indexing into the probability distributions at the indices corresponding to the correct labels.</p>
<br>
<p><strong>Note:</strong></p>
<p>The <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.arange.html"><em>torch.arange(n)</em></a> function returns a one-dimensional tensor containing values [0, 1, 2, .., n-1].</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">ys</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loss:&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss: 3.877657651901245
</pre></div>
</div>
</div>
</div>
<br>
</section>
<section id="backward-pass">
<h2>2.13 Backward pass<a class="headerlink" href="#backward-pass" title="Link to this heading">#</a></h2>
<p>With the goal of minimizing the loss function, we can use <strong>gradient-based optimization to tune the parameters</strong> of the neural network and improve its performance.</p>
<br>
<p><strong>Note:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># set the gradients to 0</span>
<span class="n">W</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>

<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The 27x27 matrix <code class="docutils literal notranslate"><span class="pre">W.grad</span></code> contains the infleunce of each weight on the loss function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.0033,  0.0037,  0.0150,  0.0019,  0.0047, -0.1986,  0.0038,  0.0080,
          0.0053,  0.0046,  0.0015,  0.0024,  0.0240,  0.0128,  0.0066,  0.0035,
          0.0015,  0.0024,  0.0122,  0.0086,  0.0024,  0.0065,  0.0096,  0.0086,
          0.0051,  0.0064,  0.0342],
        [-0.1978,  0.0196,  0.0030,  0.0359,  0.0023,  0.0032,  0.0035,  0.0116,
          0.0067,  0.0021,  0.0054,  0.0142,  0.0009,  0.0048,  0.0053,  0.0031,
          0.0201,  0.0011,  0.0005,  0.0062,  0.0180,  0.0043,  0.0028,  0.0012,
          0.0009,  0.0186,  0.0027],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0037,  0.0078,  0.0013,  0.0231,  0.0045,  0.0023,  0.0098,  0.0092,
          0.0013,  0.0026,  0.0140,  0.0080,  0.0040, -0.1827,  0.0047,  0.0040,
          0.0072,  0.0126,  0.0045,  0.0175,  0.0047,  0.0031,  0.0073,  0.0023,
          0.0137,  0.0024,  0.0070],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0031, -0.1868,  0.0081,  0.0044,  0.0246,  0.0061,  0.0046,  0.0017,
          0.0380,  0.0012,  0.0077,  0.0608,  0.0085, -0.1930,  0.0217,  0.0047,
          0.0040,  0.0212,  0.0122,  0.0063,  0.0404,  0.0122,  0.0093,  0.0068,
          0.0279,  0.0053,  0.0391],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000]])
</pre></div>
</div>
</div>
</div>
<p>We can use the gradient information in the matrix <code class="docutils literal notranslate"><span class="pre">W.grad</span></code> to update the weights of the neural network.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">W</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="all-together">
<h2>2.14 All together<a class="headerlink" href="#all-together" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="c1"># seed the random number generator for reproducibility</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">2147483647</span><span class="p">)</span>

<span class="n">max_iters</span> <span class="o">=</span> <span class="mi">100</span>


<span class="c1">#-------------------------- Create training set --------------------------#</span>

<span class="c1"># input and labels lists</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>

    <span class="c1"># list of characters</span>
    <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>

    <span class="c1"># create bigrams from the list of characters</span>
    <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>

        <span class="c1"># get the indices of the characters</span>
        <span class="n">ix1</span> <span class="o">=</span> <span class="n">ch_to_int</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
        <span class="n">ix2</span> <span class="o">=</span> <span class="n">ch_to_int</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>

        <span class="c1"># append the indices to the lists</span>
        <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix1</span><span class="p">)</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix2</span><span class="p">)</span>

<span class="c1"># convert the lists to PyTorch tensors</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>

<span class="c1"># one-hot encoding</span>
<span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="n">num</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of bigram examples: </span><span class="si">{</span><span class="n">num</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>


<span class="c1">#-------------------------- Train neural network -------------------------#</span>

<span class="c1"># initialize parameters</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="mi">27</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iters</span><span class="p">):</span>

    <span class="c1"># forward pass</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num</span><span class="p">),</span> <span class="n">ys</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">iter</span> <span class="o">==</span> <span class="n">max_iters</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Step: </span><span class="si">{</span><span class="nb">iter</span><span class="si">:</span><span class="s1">2d</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">max_iters</span><span class="si">}</span><span class="s1">   Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="c1"># backward pass</span>
    <span class="n">W</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># update parameters</span>
    <span class="n">W</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">50</span> <span class="o">*</span> <span class="n">W</span><span class="o">.</span><span class="n">grad</span>


<span class="c1">#-------------------------- Sample from neural network -------------------#</span>

<span class="nb">print</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>

    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># start character</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>

        <span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">ix</span><span class="p">]),</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># sample from the probability distribution</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="c1"># stop sequence generation when model samples end character</span>
        <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># append nex sampled character into the sequence</span>
        <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">int_to_ch</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of bigram examples: 228146

Step:  0/100   Loss: 3.7590
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 10/100   Loss: 2.6890
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 20/100   Loss: 2.5728
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 30/100   Loss: 2.5302
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 40/100   Loss: 2.5087
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 50/100   Loss: 2.4961
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 60/100   Loss: 2.4880
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 70/100   Loss: 2.4824
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 80/100   Loss: 2.4783
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 90/100   Loss: 2.4751
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 99/100   Loss: 2.4729

morvann
akela
az
arileri
chaiadayra
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ch1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 1: Neural Networks</p>
      </div>
    </a>
    <a class="right-next"
       href="references.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">References</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-data-types">2.1 Python Data Types</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-and-explore-a-text-file">2.2 Load and Explore a Text File</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mapping">2.3 Mapping</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#counts-matrix">2.4 Counts Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-matrix">2.5 Probability Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-from-a-probability-distribution">2.6 Sample From a Probability Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#average-negative-log-likelihood">2.7 Average Negative Log Likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-set">2.8 Training Set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding">2.9 One-hot Encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">1.6 Softmax</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">2.12 Loss function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-pass">2.13 Backward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#all-together">2.14 All together</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Daniel Simon
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>