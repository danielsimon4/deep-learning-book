{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Import Dataset\n",
    "\n",
    "Usually data is not ready\n",
    "\n",
    "Using the MNIST dataset, I created a CNN model that recognizes handwritten digits with an accuracy of 99.64%. That is, it correctly predicted 4,972 out of 5,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "\n",
    "# set print options to avoid scientific notation\n",
    "torch.set_printoptions(sci_mode=False, precision=2)\n",
    "\n",
    "# load the MNIST dataset\n",
    "ds = load_dataset(\"ylecun/mnist\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dataset structure, we can access the images like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uu8GeBj4w07XrpdTS1bSrU3IiMRczcMcZyNo+Xrz16VyNFFex/s8TfaPEut6NMiPZXmms8yEcttdVxn0xI36V5PqkSQateQxLtjjndVX0AYgCqlFehfBTXl0L4l2QkIWG/RrJ2Pbdgr/AOPKtUvi3pcOkfFDW7e3CiN5VnCquADIgcj82NcVRRRRX//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAmklEQVR4AWNgGGAw8f9leVxOUHj79487siQTEuf1ISQOiIks+fUhHkkBfTySXHIMDKY4XVT798+fHDTdCC6aJLKDgM5jZESoZGBgQeYw/PuPwkXViSKF6k80KXRJoJ12GEpgAkDX/tGCcdB1zgBKpOGSvIGQwMK69ffvf2W4OJpXrv7//w8uhxJlQNFZCBkgC03ntesosgPAAQDnIiOtEX7BlwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][6]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APAACzBVBJJwAO9dnp/wm8damu6Dw5dRjGf9IKw/+hkVPffCnWNJa7XVNV0Kxa1hErrNe/M2cnYqgElsAHpjkc1wlAODkV694W8c654t8M6n4TuvEctrrFw0cun3c0/lq+3AMJcDK5AyOeTkd+fPvGFn4gsvEtzF4m89tUG1ZJJjuMgUBVYN/EMKOe9YVXtK0bUtdvVs9LsZ7y4YgbIULYycZPoPc8V6lpfwh0/w7p66z8RdXj0y2z8llC4aWQ+mRn8lz9RXPfE3x1pvi46TYaPZTQadpMJghluWDSyrhQM9SMBe5Oc5NcBV7Tda1XRZJJNK1O8sXkG12tZ2iLD0JUjNQ3l9eahN517dT3MvTfNIXb16n6mq9Ff/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4AWNgGMyAWUhIqK5jvdSy/9/rQe5kgTlWjs3KRiAYxHsyKfDzxYMgFiOIAALDvfwQBsO/pK8Mz97fhPLAlNDtvyBwbNv3j8jCUHbAnOy/f89yM2jPwiLJwMc4628UqgQTnPvp/0eGFAQXLg5lcO/764YuhuArf3y4IAfmfoQwlBX44e/fckkMYaiA7q6/f6dJ45IViP3zdzcuSQaGn39/OkBl4WEL4euFmLIwXDuETav6lKfAIPy1DYucRNFdUPCe9MOUE3e6CpI6FogZSEKrwbFyOIATQ5v5mkcgXV9auVGlwK4NDGRguL75b88HVDla8QBFF16ADQA8sQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_image = ds['train'][0]['image']\n",
    "first_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the images in the dataset are in PIL (Python Imaging Library) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(first_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Usually, the collected data is not immediately ready to be the input of a machine learning model. The steps taken to clean and prepare the data are known as **data preprocessing**.\n",
    "\n",
    "When working with PIL images in deep learning, the typical workflow involves converting them first to NumPy arrays and then, to normalized PyTorch tensors. \n",
    "\n",
    "Suppose we have the following 4x4 PIL image:\n",
    "\n",
    "```{figure} ../images/image-example.png\n",
    "---\n",
    "width: 180px\n",
    "name: image-example\n",
    "---\n",
    "4x4 PIL image\n",
    "```\n",
    "\n",
    "When we convert this image to a NumPy array, each pixel value ranges from 0 (black) to 255 (white), with intermediate values representing varying shades of gray.\n",
    "\n",
    "```python\n",
    "np.array([[  0,  42,  85, 127],\n",
    "          [ 42,  85, 127, 170],\n",
    "          [ 85, 127, 170, 213],\n",
    "          [127, 170, 213, 255]])\n",
    "```\n",
    "\n",
    "Neural networks perform better when when we **normalize** the input values so that they fall within the range [0, 1]. To do this, we divide each pixel value by 255, transforming the image into a format where 0 represents black and 1 represents white.\n",
    "\n",
    "```python\n",
    "tensor([[0.0000, 0.1647, 0.3333, 0.4980],\n",
    "        [0.1647, 0.3333, 0.4980, 0.6667],\n",
    "        [0.3333, 0.4980, 0.6667, 0.8353],\n",
    "        [0.4980, 0.6667, 0.8353, 1.0000]])\n",
    "```\n",
    "\n",
    "We will now apply these transformations to all the images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PIL image to normalized PyToch tensor\n",
    "def image_to_tensor(image):\n",
    "    return torch.tensor(np.array(image)) / 255.0\n",
    "\n",
    "\n",
    "def preprocess_data(split):\n",
    "    \n",
    "    x = []  # list to store image tensors\n",
    "    y = []  # list to store labels\n",
    "\n",
    "    for example in split:\n",
    "        x.append(image_to_tensor(example['image']))\n",
    "        y.append(example['label'])\n",
    "    \n",
    "    return torch.stack(x), torch.tensor(y)\n",
    "\n",
    "train_x, train_y = preprocess_data(ds['train'])\n",
    "test_x,  test_y  = preprocess_data(ds['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test, and Validation Splits\n",
    "\n",
    "When working with neural networks, it is common practice to divide the dataset into three splits:\n",
    "\n",
    "- The **train split** (80% of the dataset) is used to optimize the model parameters.\n",
    "- The **validation split** (10% of the dataset) is used to optimize the hyperparameters, such as the size of the hidden layer, embedding size, and regularization strength.\n",
    "- The **test split** (10% of the dataset) is used to evaluate the final performance of the model.\n",
    "\n",
    "```{important}\n",
    "The test split should be used only after the model is fully optimized, as its goal is to evaluate the model's performance on new data. Attempting to improve the test loss adjusting the hyperparameters of the model would result in overfitting.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "(2.6)=\n",
    "## Convolutional Neural Network\n",
    "\n",
    "A **Convolutional Neural Network (CNN)** is a type of neural network primarly designed for tasks like image recognition and object detection.\n",
    "\n",
    "CNNs outperform traditional neural netowrks when working with images becuase:\n",
    "- They are more efficient, it is too much computation to train large-size images with different channels.\n",
    "- They can capture the spatial dependencies of the image.\n",
    "- They are not sensitive to the position of the object in the image.\n",
    "\n",
    "\n",
    "CNNs works in two steps:\n",
    "- **Feature extraction** is a phase where various filters and layers are applied to the images to extract information and features.\n",
    "- **Classification** is a pahse where the image is classified based on the information extracted.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "(2.7)=\n",
    "## Convoultion Layer\n",
    "\n",
    "The **convolution layer** extracts different features from the input image by applying different **filters** and producing several **feature maps**. \n",
    "\n",
    "```{figure} ../images/conv-layer-1.png\n",
    "---\n",
    "width: 340px\n",
    "name: conv-layer-1\n",
    "---\n",
    "Convoultional Layer\n",
    "```\n",
    "\n",
    "To understand the math behind convolution layers, we will look at only one filter.\n",
    "\n",
    "```{figure} ../images/conv-layer-2.png\n",
    "---\n",
    "width: 480px\n",
    "name: conv-layer-2\n",
    "---\n",
    "Convoultional Layer 2\n",
    "```\n",
    "\n",
    "To calculate $m_{11}$, a fetaure map value, we will multiply the pixel values of the top-left 3 x 3 subregion of the input image with the filter values and add them up (sum product).\n",
    "\n",
    "$$\n",
    "m_{11} = i_{11}f_{11} + i_{12}f_{12} + i_{13}f_{13} + i_{21}f_{21} + i_{22}f_{22} + i_{23}f_{23} + i_{31}f_{31} + i_{32}f_{32} + i_{33}f_{33}\n",
    "$$\n",
    "\n",
    "To calculate $m_{12}$, we will select a 3 x 3 subregion one column to the right.\n",
    "\n",
    "$$\n",
    "m_{12} = i_{12}f_{11} + i_{13}f_{12} + i_{14}f_{13} + i_{22}f_{21} + i_{23}f_{22} + i_{24}f_{23} + i_{32}f_{31} + i_{33}f_{32} + i_{34}f_{33}\n",
    "$$\n",
    "\n",
    "The rest of the feature map values are calculated moving the 3x3 subregion similarlt. Thus, a convolution can be expressed as:\n",
    "$$\n",
    "m_{ij} = {f} \\cdot {i}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $i$ is the corresponfing subregion of the input\n",
    "- $f$ is the filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random number generator for reproducibility\n",
    "g = torch.Generator().manual_seed(1)\n",
    "\n",
    "# intitialize input image randomly\n",
    "image = torch.randn((6,6), generator=g)\n",
    "\n",
    "\n",
    "n_filters = 5\n",
    "filter_size = (3,3)\n",
    "\n",
    "# intitialize filters randomly\n",
    "filters = torch.randn((n_filters, filter_size[0], filter_size[1]), generator=g)\n",
    "\n",
    "# dimensions of the feature maps\n",
    "map_size = (image.shape[0] - filter_size[0] + 1, image.shape[1] - filter_size[1] + 1)\n",
    "\n",
    "# initialize with zeros the feature maps\n",
    "maps = torch.zeros((n_filters, map_size[0], map_size[1]))\n",
    "\n",
    "# apply convolution\n",
    "for i in range(map_size[0]):\n",
    "\n",
    "    for j in range(map_size[1]):\n",
    "\n",
    "        # extract the subregion of the input image\n",
    "        subregion = image[i:i+filter_size[0], j:j+filter_size[1]]\n",
    "\n",
    "        # update feature map at position (i,j) for all filters\n",
    "        maps[:, i, j] = filters.view(-1, 9) @ subregion.reshape(-1)\n",
    "\n",
    "# apply ReLU activation\n",
    "maps = torch.relu(maps)\n",
    "\n",
    "maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "## Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_size = (2,2)       #  size of the pooling window\n",
    "\n",
    "# dimensions of the feature maps after pooling\n",
    "pooled_map_size = (maps.shape[1] // pool_size[0], maps.shape[2] // pool_size[1])\n",
    "\n",
    "# initialize with zeros the pooled feature maps\n",
    "pooled_maps = torch.zeros((n_filters, pooled_map_size[0], pooled_map_size[1]))\n",
    "\n",
    "# apply max pooling\n",
    "for i in range(pooled_map_size[0]):\n",
    "\n",
    "    for j in range(pooled_map_size[1]):\n",
    "\n",
    "        # Extract the 2x2 region for max pooling\n",
    "        subregions = maps[:, \n",
    "                         i * pool_size[0]:(i+1) * pool_size[0], \n",
    "                         j * pool_size[1]:(j+1) * pool_size[1]]\n",
    "        \n",
    "        # Apply max pooling\n",
    "        pooled_maps[:, i, j] = subregions.reshape(n_filters, -1).max(dim=1).values\n",
    "\n",
    "pooled_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 30\n",
    "filter_size = (5, 5)\n",
    "pool_size = (2, 2)\n",
    "n_hidden = 500      # number of neurons in hidden layers\n",
    "\n",
    "\n",
    "# intitialize filter, weight matrices and bias vectors randomly\n",
    "g = torch.Generator().manual_seed(1)\n",
    "\n",
    "f1 = torch.randn((n_filters, filter_size[0], filter_size[1]), generator=g)\n",
    "\n",
    "W1 = torch.randn((4320, n_hidden), generator=g)* 0.01\n",
    "b1 = torch.randn(n_hidden, generator=g)\n",
    "\n",
    "W2 = torch.randn((n_hidden, 10), generator=g)* 0.01\n",
    "b2 = torch.randn(10, generator=g)\n",
    "\n",
    "# parameters\n",
    "parameters = [f1, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True                     # track gradients\n",
    "\n",
    "print(f\"Number of parameters: {sum(p.nelement() for p in parameters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100   # train iterations\n",
    "lr = 0.01         # learning rate\n",
    "batch_size = 32   # number of examples per batch\n",
    "\n",
    "track_loss = []\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # batch construct\n",
    "    ix = torch.randint(0, train_x.shape[0], (batch_size,))\n",
    "    batch = train_x[ix]\n",
    "\n",
    "    # apply convolution\n",
    "    map_size = (batch.shape[1] - filter_size[0] + 1, batch.shape[2] - filter_size[1] + 1)\n",
    "    maps = torch.zeros((batch_size, n_filters, map_size[0], map_size[1]))\n",
    "\n",
    "    for i in range(map_size[0]):\n",
    "        \n",
    "        for j in range(map_size[1]):\n",
    "\n",
    "            # extract the subregion for all images in the batch\n",
    "            subregion = batch[:, i:i + filter_size[0], j:j + filter_size[1]]\n",
    "\n",
    "            maps[:, :, i, j] = subregion.reshape(batch_size, -1) @ f1.view(-1, n_filters) # (32, 30, 24, 24) = (32, 25) @ (25, 30)\n",
    "    \n",
    "    maps = torch.relu(maps)\n",
    "    \n",
    "    # apply pool\n",
    "    pooled_map_size = (maps.shape[2] // pool_size[0], maps.shape[3] // pool_size[1])\n",
    "    pooled_maps = torch.zeros((batch_size, n_filters, pooled_map_size[0], pooled_map_size[1]))\n",
    "\n",
    "    for i in range(pooled_map_size[0]):\n",
    "\n",
    "        for j in range(pooled_map_size[1]):\n",
    "\n",
    "            # extract the subregion for all images in the batch and filters\n",
    "            subregion = maps[:, :, i * pool_size[0]:(i+1) * pool_size[0], j * pool_size[1]:(j+1) * pool_size[1]]\n",
    "\n",
    "            pooled_maps[:, :, i, j] = subregion.reshape(batch_size, n_filters, -1).max(dim=2).values # (32, 30, 12, 12)\n",
    "    \n",
    "    # forward pass\n",
    "    h1 = torch.relu(pooled_maps.view(batch_size, -1) @ W1 + b1) # (32, 100) = (32, 4320) x (4320, 100) + (100)\n",
    "    h2 = torch.relu(h1 @ W2 + b2)                               # (32, 10) = (32, 100) x (100, 10) + (10)           \n",
    "\n",
    "    # calculate loss\n",
    "    loss = F.cross_entropy(h2, train_y[ix])\n",
    "    track_loss.append(loss.item())\n",
    "    if step % 3 == 0 or step == max_steps - 1:\n",
    "        print(f\"Step: {step:2d}/{max_steps}     Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-torch.tensor(1/10.0).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After Convolution: {maps.mean().item()}, After ReLU: {maps.relu().mean().item()}\")\n",
    "print(f\"After Hidden Layer 1: {h1.mean().item()}, After Hidden Layer 2: {h2.mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(track_loss[10:])\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
