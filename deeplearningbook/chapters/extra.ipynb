{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "\n",
    "The **Mean Squared Error (MSE)** is a commonly used loss function for regression tasks, where the objective is to predict continuous values. MSE computes the average of the squared differences between the predicted values ($\\hat{y}_i$) and actual values($y_i$). The squaring of the errors results in larger penalties for bigger discrepancies, making MSE particularly sensitive to outliers compared to the **Mean Absolute Error (MAE)**, which treats all errors equally.\n",
    "\n",
    "The formulas for MSE and MAE are as follows:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "We will first define the following targets, which are the values to which the neural network's output should converge.\n",
    "\n",
    "We will then calculate the loss using the Mean Squared Error.\n",
    "\n",
    "```{note}\n",
    "The `tensor.item()` method returns the value of a single-element tensor. For more information, please refer to the [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# calculate loss using MSE\n",
    "loss = ((h2 - targets) ** 2).mean()\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.1 # learning rate\n",
    "\n",
    "# clone for later comparison \n",
    "old_W1 = W1.data.clone()\n",
    "old_W2 = W2.data.clone()\n",
    "\n",
    "# update parameters using gradient descent\n",
    "for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "print(f\"W1 before GD:\\n{old_W1}\")\n",
    "print(f\"W1 gradients:\\n{W1.grad}\")\n",
    "print(f\"W1 after GD:\\n{W1.data}\\n\")\n",
    "\n",
    "print(f\"W2 before GD:\\n{old_W2}\")\n",
    "print(f\"W2 gradients:\\n{W2.grad}\")\n",
    "print(f\"W2 after GD2:\\n{W2.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```{note}\n",
    "Please note that the gradients indicate whether the values should be increased or decreased, as well as the magnitude of the adjustment.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, through gradient descent, the neural network gradually reduced its loss and improved its predictions over time. Comparing the initial output to the final output, we can observe that the final predictions are closer to the target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "As we saw in section [](1.3), a **bias** is a parameter added to the output of a neuron that enables the model to learn more complex patterns.\n",
    "\n",
    "- The bias term allows the model to adjust the neuron's output independently of its inputs, helping the model better fit the training data and capture more complex patterns.\n",
    "\n",
    "Bias are important for 2 main reasons:\n",
    "\n",
    "1. **Flexibility**: Bias introduces an extra degree of freedom that enables the network to shift the activation function, making it more flexible and capable of learning a broader range of functions. Without bias, the output of a neuron is strictly dependent on the weighted sum of its inputs.\n",
    "\n",
    "2. **Preventing Overfitting**: In some cases, bias helps avoid overfitting by allowing the network to generalize better, especially in situations where the data is not perfectly centered or the output doesn't naturally pass through the origin (zero)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
