{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Neural Networks\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.1)=\n",
    "## 1.1 Deep Learning\n",
    "\n",
    "**Artificial Intelligence (AI)** is a branch of computer science focused on creating systems that perform tasks requiring **human-like intelligence**, such as language comprehension, pattern recognition, problem-solving, and decision-making. AI aims to enable machines to perform complex tasks in ways that mimic human reasoning and adaptability.\n",
    "\n",
    "**Machine Learning (ML)** is a subset of AI that involves **training algorithms on data** to identify patterns and make predictions. ML models learn from data and improve their accuracy over time, typically using one of three main approaches:\n",
    "- **Supervised Learning**: The model is trained on labeled data, where each input is paired with a known output. The model learns to associate inputs with outputs, making it well-suited for tasks such as classification (e.g., image recognition) and regression (e.g., predicting prices).\n",
    "- **Unsupervised Learning**: The model is trained on unlabeled data, without predefined outputs. This approach is used to discover hidden patterns or groupings within the data, commonly applied in clustering and association tasks.\n",
    "- **Reinforcement Learning**: The model learns through feedback from rewards and penalties based on its actions. Reinforcement learning is often applied in environments where decision-making is complex, such as strategic games (e.g., chess) and robotics, where learning occurs via trial and error.\n",
    "\n",
    "**Deep Learning (DL)** is a specialized area within ML that uses **neural networks** to recognize complex patterns in large datasets.\n",
    "\n",
    "```{figure} ../images/deep-learning.png\n",
    "---\n",
    "width: 140px\n",
    "name: deep-learning\n",
    "---\n",
    "Deep Learning Overview\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.2)=\n",
    "## 1.2 Neural Networks\n",
    "\n",
    "**Neural networks** are computational models inspired by the structure of the human brain, designed to recognize patterns and make predictions. They consist of **layers of interconnected nodes** (often called neurons) that process information through mathematical operations. \n",
    "\n",
    "A basic neural network has the following structure: \n",
    "- **Input Layer**: This first layer receives raw data, like images, text, or numerical values. Each node in this layer represents an **input feature**.\n",
    "- **Hidden Layers**: These intermediate layers between the input and output layers **process information**. Each hidden layer transforms data from the previous layer, allowing the network to progressively learn and recognize patterns.\n",
    "- **Output Layer**: This final layer provides the network’s **output**, such as classifying an image or predicting a value.\n",
    "\n",
    "```{figure} ../images/neural-network.png\n",
    "---\n",
    "width: 340px\n",
    "name: neural-network\n",
    "---\n",
    "Basic Structure of a Neural Network\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "(1.3)=\n",
    "## 1.3 Neurons\n",
    "\n",
    "In a neural network, each **neuron** is a fundamental unit that takes in **multiple inputs** and processes them to produce a **single output**. As shown in [*Fig. 2 Basic Structure of a Neural Network*](neural-network), each neuron in the hidden and output layers connects to all the neurons in the previous layer. These connections have associated values known as **weights** that represent the strength of the connection between the neurons. \n",
    "\n",
    "When an input reaches a neuron, it is multiplied by the weight of its connection, and the results are combined (summed up). An additional value, called a **bias** (b), may be added to adjust the sum. The result may also be passed through an **activation function** (σ), which determines the neuron's output by introducing non-linearity.\n",
    "\n",
    "```{figure} ../images/neuron.png\n",
    "---\n",
    "width: 250px\n",
    "name: neuron\n",
    "---\n",
    "Structure of a Neuron\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "The output of the above neuron would be:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\sigma\\left(w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b\\right)\n",
    "$$\n",
    "\n",
    "Thus, in general, the output of a neuron can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\sigma\\left(Σ_{j} w_{j} x_{j} + b\\right)\n",
    "$$\n",
    "\n",
    "Using matrix multiplication, the above expression can be represented as:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\sigma({w} \\cdot {x} + b)\n",
    "$$\n",
    "\n",
    "```{important}\n",
    "The output of the above neuron would be one of the inputs for the neurons in the next layer, and so on, allowing the neural network to learn complex patterns in the data through layers of transformations.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "Please note that a larger weight indicates a stronger connection between the neurons, while the bias term allows the activation of a neuron to be adjusted independently of its inputs, enabling the model to better fit the training data and capture more complex patterns.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.4)=\n",
    "## 1.4 PyTorch\n",
    "\n",
    "We will use **PyTorch** to build our neural networks. PyTorch is an open-source machine learning library widely used for building and training deep learning models due to its flexibility, ease of use, and efficient computation. It provides multi-dimensional arrays, known as **tensors**, which are similar to NumPy arrays but optimized for GPU processing. Depending on their dimensions, we will refer to the tensors differently.\n",
    "\n",
    "A 1-dimensional tensor is called a **vector**. A vector with shape (3) would look like this:\n",
    "\n",
    "```python\n",
    "tensor([1, 2, 3])\n",
    "```\n",
    "\n",
    "A 2-dimensional tensor is called a **matrix**: A matrix with shape (2, 3) would look like this:\n",
    "\n",
    "```python\n",
    "tensor([[1, 2, 3],\n",
    "        [4, 5, 6]])\n",
    "```\n",
    "\n",
    "A tensor with 3 or more dimensions is called an **n-dimensional tensor**. A 3D tensor with shape (2, 2, 3) would look like this:\n",
    "\n",
    "```python\n",
    "tensor([[[1, 2, 3],\n",
    "         [4, 5, 6]],\n",
    "        [[7, 8, 9],\n",
    "         [10, 11, 12]]])\n",
    "```\n",
    "\n",
    "The following code will set up the environment for working with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch library\n",
    "import torch\n",
    "\n",
    "# set print options to avoid scientific notation\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "(1.5)=\n",
    "## 1.5 Understanding How Neural Networks Learn\n",
    "\n",
    "Neural networks use supervised learning to **fine-tune their parameters**. In other words, neural networks learn by adjusting thier weights and biases through an iterative process that involves four key steps:\n",
    "1. **Forward pass**: The input data flows through the network layer by layer, producing an output.\n",
    "2. **Loss Function**: The network's output is compared to the actual values, and a loss function (like MSE or Cross-Entropy) is used to measure the prediction error.\n",
    "3. **Backpropagation**: The network calculates gradients by propagating the error backward through the layers, determining how much each parameter contributed to the error.\n",
    "4. **Update Parameters**: The calculated gradients are used to adjust the weights and biases through an optimization algorithm (like Gradient Descent, SGD, or Adam) to minimize the loss function.\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.6)=\n",
    "## 1.6 Forward Pass\n",
    "\n",
    "We will implement a **forward pass** using 5 input examples for the neural network illustrated in [*Fig. 2 Basic Structure of a Neural Network*](neural-network). This network had 3 input features, 2 hidden layers with 6 neurons each, and 2 output neurons. \n",
    "\n",
    "We will first initialize the network parameters. For simplicity, we will not include any bias or activation functions at this stage.\n",
    "\n",
    "```{note}\n",
    "- The `torch.randn` function generates a tensor filled with random numbers drawn from a standard normal distribution (mean = 0, standard deviation = 1). For more information, please refer to the [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.randn.html).\n",
    "- The `requires_grad=True` argument indicates that the tensor should track gradients for operations. This will enable to run `loss.backward()` during backpropagation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 30\n"
     ]
    }
   ],
   "source": [
    "# seed the random number generator for reproducibility\n",
    "g = torch.Generator().manual_seed(1)\n",
    "\n",
    "# intitialize randomly weight matrices\n",
    "W1 = torch.randn((3, 6), generator=g)   # (input_to_layer, output_from_layer)\n",
    "W2 = torch.randn((6, 2), generator=g)   # (input_to_layer, output_from_layer)\n",
    "\n",
    "# list of parameters\n",
    "parameters = [W1, W2]\n",
    "\n",
    "# track gradients\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# print total number of parameters\n",
    "print(f\"Number of parameters: {sum(p.nelement() for p in parameters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network parameters have beeen intialized, we can perform a forward pass. Please note that **matrix multiplication** allows us to efficiently evaluate the output for the 5 input examples in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[ 0.6614,  0.2669,  0.0617],\n",
      "        [ 0.6213, -0.4519, -0.1661],\n",
      "        [-1.5228,  0.3817, -1.0276],\n",
      "        [-0.5631, -0.8923, -0.0583],\n",
      "        [-0.1955, -0.9656,  0.4224]])\n",
      "\n",
      "Output:\n",
      "tensor([[ 1.0329, -1.2264],\n",
      "        [ 2.3319,  2.0953],\n",
      "        [-2.8955, -3.1665],\n",
      "        [ 0.1669,  4.3554],\n",
      "        [ 0.5632,  5.2940]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# seed the random number generator for reproducibility\n",
    "g = torch.Generator().manual_seed(1)\n",
    "\n",
    "# intitialize randomly 5 input examples\n",
    "x = torch.randn((5, 3), generator=g)    # (num_examples, input_features)\n",
    "\n",
    "# matrix multiplication\n",
    "h1 = x @ W1     # (5,6) = (5,3) x (3,6)\n",
    "h2 = h1 @ W2    # (5,2) = (5,6) x (6,2)\n",
    "\n",
    "# clone for later comparison \n",
    "old_h2 = h2.data.clone()\n",
    "\n",
    "# print input and output matrices\n",
    "print(f\"Input:\\n{x}\\n\")\n",
    "print(f\"Output:\\n{h2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "(1.7)=\n",
    "## 1.7 Loss Function\n",
    "\n",
    "A **loss function** is a mathematical representation that quantifies how well a machine learning model is performing. It measures the difference between the model's predicted outputs and the actual outputs from the dataset (the **targets**).\n",
    "\n",
    "There are various types of loss functions, each suitable for different tasks:\n",
    "- Regression Tasks: Mean Squared Error (MSE) and Mean Absolute Error (MAE) (both covered in section [](1.8)).\n",
    "- Classification Tasks: Cross-Entropy Loss (covered in [](2.7)) and Hinge Loss.\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.8)=\n",
    "## 1.8 Mean Squared Error (MSE)\n",
    "\n",
    "The **Mean Squared Error (MSE)** is a commonly used loss function for regression tasks, where the objective is to predict continuous values. MSE computes the average of the squared differences between the predicted values ($\\hat{y}_i$) and actual values($y_i$). The squaring of the errors results in larger penalties for bigger discrepancies, making MSE particularly sensitive to outliers compared to the **Mean Absolute Error (MAE)**, which treats all errors equally.\n",
    "\n",
    "The formulas for MSE and MAE are as follows:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "We will first define the following targets, which are the values to which the neural network's output should converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets:\n",
      "tensor([[ 1.1000, -1.3000],\n",
      "        [ 2.3000,  2.0000],\n",
      "        [-2.5000, -3.2000],\n",
      "        [ 0.2000,  4.3000],\n",
      "        [ 0.6000,  5.3000]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets = torch.tensor([[1.1, -1.3], [2.3, 2], [-2.5, -3.2], [0.2, 4.3], [0.6, 5.3]])\n",
    "\n",
    "print(f\"Targets:\\n{targets}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then calculate the loss using the Mean Squared Error.\n",
    "\n",
    "```{note}\n",
    "The `tensor.item()` method returns the value of a single-element tensor. For more information, please refer to the [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0183\n"
     ]
    }
   ],
   "source": [
    "# calculate loss using MSE\n",
    "loss = ((h2 - targets) ** 2).mean()\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "(1.9)=\n",
    "## 1.9 Backpropagation\n",
    "\n",
    "During **backpropagation**, the neural network computes the **gradient of the loss function** with respect to all its parameters by applying the chain rule of calculus.\n",
    "\n",
    "To compute all the gradients, we will use the PyTorch function `loss.backward()` which kept track of all the operations during the forwards pass. In  [](ch3.ipynb) we will perform a backward pass manually to better understand how backpropagation works and how the gradients are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the gradients to None\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "# calculate gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "(1.10)=\n",
    "## 1.10 Update Parameters\n",
    "\n",
    "Once the gradients have been computed, the neural network progresively **updates its parameters** trying minimize the loss function. The most common optimization techniques for updating the parameters are **gradient descent**, **stochastic gradient descent (SGD)**, and **Adam**.\n",
    "\n",
    "A **gradient** is a vector that represents the rate of change of a function with respect to its input variables. By default, gradients point in the **direction of steepest ascent**, that is, the direction in which the function increases the fastest. Since we want to minimize the loss function, we will move in the opposite direction of the gradients. \n",
    "\n",
    "<br>\n",
    "\n",
    "(1.11)=\n",
    "## 1.11 Gradient Descent\n",
    "\n",
    "**Gradient descent** is an optimization algorithm used to minimize a function, frequently used in neural networks training to minimize the loss function.\n",
    "\n",
    "Gradient descent updates each parameter by subtracting a fraction of the gradient from its current value, scaled by a factor known as the **learning rate**. The learning rate determines the size of the steps we take towards the minimum. A smaller learning rate results in more precise but slower convergence, while a larger learning rate can speed up convergence but may risk overshooting the minimum. The parameter update rule for gradient descent can be expressed mathematically as:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\nabla L(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ represents the parameters of the neural network\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\nabla L(\\theta)$ is the gradient of the loss function with respect to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 before GD:\n",
      "tensor([[-1.5256, -0.7502,  0.6995,  0.1991,  0.8657,  0.2444],\n",
      "        [-0.6629,  0.8073,  0.4391,  1.1712, -2.2456, -1.4465],\n",
      "        [ 0.0612, -0.6177, -0.7981, -0.1316, -0.7984,  0.3357]])\n",
      "W1 gradients:\n",
      "tensor([[ 0.0536, -0.0771,  0.2550,  0.1593,  0.0444,  0.2188],\n",
      "        [-0.0216,  0.0368, -0.0499, -0.0180, -0.0137, -0.0411],\n",
      "        [ 0.0185, -0.0187,  0.1620,  0.1197,  0.0213,  0.1415]])\n",
      "W1 after GD:\n",
      "tensor([[-1.5310, -0.7425,  0.6740,  0.1832,  0.8613,  0.2225],\n",
      "        [-0.6608,  0.8036,  0.4441,  1.1730, -2.2442, -1.4423],\n",
      "        [ 0.0593, -0.6159, -0.8143, -0.1436, -0.8006,  0.3216]])\n",
      "\n",
      "W2 before GD:\n",
      "tensor([[ 0.3935,  1.1322],\n",
      "        [-0.5404, -2.2102],\n",
      "        [ 2.1130, -0.0040],\n",
      "        [ 1.3800, -1.3505],\n",
      "        [ 0.3455,  0.5046],\n",
      "        [ 1.8213, -0.1814]])\n",
      "W2 gradients:\n",
      "tensor([[-0.1637, -0.0016],\n",
      "        [-0.1570, -0.0064],\n",
      "        [ 0.0129,  0.0072],\n",
      "        [-0.0138, -0.0103],\n",
      "        [ 0.0964,  0.0373],\n",
      "        [ 0.0894,  0.0136]])\n",
      "W2 after GD2:\n",
      "tensor([[ 0.4099,  1.1324],\n",
      "        [-0.5247, -2.2096],\n",
      "        [ 2.1117, -0.0047],\n",
      "        [ 1.3814, -1.3495],\n",
      "        [ 0.3358,  0.5009],\n",
      "        [ 1.8124, -0.1828]])\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1 # learning rate\n",
    "\n",
    "# clone for later comparison \n",
    "old_W1 = W1.data.clone()\n",
    "old_W2 = W2.data.clone()\n",
    "\n",
    "# update parameters using gradient descent\n",
    "for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "print(f\"W1 before GD:\\n{old_W1}\")\n",
    "print(f\"W1 gradients:\\n{W1.grad}\")\n",
    "print(f\"W1 after GD:\\n{W1.data}\\n\")\n",
    "\n",
    "print(f\"W2 before GD:\\n{old_W2}\")\n",
    "print(f\"W2 gradients:\\n{W2.grad}\")\n",
    "print(f\"W2 after GD2:\\n{W2.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```{note}\n",
    "Please note that the gradients indicate whether the values should be increased or decreased, as well as the magnitude of the adjustment.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.11)=\n",
    "## 1.11 Neural Network Training\n",
    "\n",
    "During **training**, a neural network iteratively makes a forward pass, calcules the loss, makes a backward pass and updates its parameters. Each iteration is usually called a **training step**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 / 15   Loss: 0.0062\n",
      "Step:  1 / 15   Loss: 0.0054\n",
      "Step:  2 / 15   Loss: 0.0051\n",
      "Step:  3 / 15   Loss: 0.0049\n",
      "Step:  4 / 15   Loss: 0.0047\n",
      "Step:  5 / 15   Loss: 0.0045\n",
      "Step:  6 / 15   Loss: 0.0044\n",
      "Step:  7 / 15   Loss: 0.0043\n",
      "Step:  8 / 15   Loss: 0.0042\n",
      "Step:  9 / 15   Loss: 0.0041\n",
      "Step: 10 / 15   Loss: 0.0040\n",
      "Step: 11 / 15   Loss: 0.0040\n",
      "Step: 12 / 15   Loss: 0.0039\n",
      "Step: 13 / 15   Loss: 0.0039\n",
      "Step: 14 / 15   Loss: 0.0039\n"
     ]
    }
   ],
   "source": [
    "max_steps = 15\n",
    "lr = 0.1 # learning rate\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # forward pass\n",
    "    h1 = x @ W1     # (5,6) = (5,3) x (3,6)\n",
    "    h2 = h1 @ W2    # (5,2) = (5,6) x (6,2)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = ((h2 - targets) ** 2).mean()\n",
    "\n",
    "    # set the gradients to 0\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    # calculate gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    print(f\"Step: {step:2d} / {max_steps}   Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, through gradient descent, the neural network gradually reduced its loss and improved its predictions over time. Comparing the initial output to the final output, we can observe that the final predictions are closer to the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First output:\n",
      "tensor([[ 1.0329, -1.2264],\n",
      "        [ 2.3319,  2.0953],\n",
      "        [-2.8955, -3.1665],\n",
      "        [ 0.1669,  4.3554],\n",
      "        [ 0.5632,  5.2940]])\n",
      "Last output:\n",
      "tensor([[ 0.9563, -1.2406],\n",
      "        [ 2.2978,  2.0103],\n",
      "        [-2.5604, -3.1827],\n",
      "        [ 0.2316,  4.3254],\n",
      "        [ 0.5060,  5.2981]], grad_fn=<MmBackward0>)\n",
      "Targets:\n",
      "tensor([[ 1.1000, -1.3000],\n",
      "        [ 2.3000,  2.0000],\n",
      "        [-2.5000, -3.2000],\n",
      "        [ 0.2000,  4.3000],\n",
      "        [ 0.6000,  5.3000]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial output:\\n{old_h2}\")\n",
    "print(f\"Final output:\\n{h2}\")\n",
    "print(f\"Targets:\\n{targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
