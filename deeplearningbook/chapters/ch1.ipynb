{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Deep Learning\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.1)=\n",
    "## 1.1 What is Deep Learning?\n",
    "\n",
    "\n",
    "**Artificial Intelligence (AI)** is a branch of computer science focused on creating systems that perform tasks requiring **human-like intelligence**, such as language comprehension, pattern recognition, problem-solving, and decision-making. AI aims to enable machines to perform complex tasks in ways that mimic human reasoning and adaptability.\n",
    "\n",
    "**Machine Learning (ML)** is a subset of AI that involves **training algorithms on data** to identify patterns and make predictions. ML models learn from data and improve their accuracy over time, typically using one of three main approaches:\n",
    "- **Supervised Learning**: The model is trained on labeled data, where each input is paired with a known output. The model learns to associate inputs with outputs, making it well-suited for tasks such as classification (e.g., image recognition) and regression (e.g., predicting prices).\n",
    "- **Unsupervised Learning**: The model is trained on unlabeled data, without predefined outputs. This approach is used to discover hidden patterns or groupings within the data, commonly applied in clustering and association tasks.\n",
    "- **Reinforcement Learning**: The model learns through feedback from rewards and penalties based on its actions. Reinforcement learning is often applied in environments where decision-making is complex, such as strategic games (e.g., chess) and robotics, where learning occurs via trial and error.\n",
    "\n",
    "**Deep Learning (DL)** is a specialized area within ML that uses **neural networks** to recognize complex patterns in large datasets.\n",
    "\n",
    "<div style=\"width: 400px; margin: 0 auto;\">\n",
    "   <img src=\"https://i0.wp.com/www.phdata.io/wp-content/uploads/2022/03/Data-Science-Terms-You-Should-Know-The-Difference-Between-AI-ML-and-DL-Image-1.png\">\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.2)=\n",
    "## 1.2 Neural Networks\n",
    "\n",
    "**Neural networks** are computational models inspired by the structure of the human brain, designed to recognize patterns and make predictions. They consist of **layers of interconnected nodes** (often called neurons) that process information through mathematical operations.\n",
    "\n",
    "A basic neural network has the following structure: \n",
    "- **Input Layer**: This first layer receives raw data, like images, text, or numerical values. Each node in this layer represents an **input feature**.\n",
    "- **Hidden Layers**: These intermediate layers between the input and output layers **process information**. Each hidden layer transforms data from the previous layer, allowing the network to progressively learn and recognize patterns.\n",
    "- **Output Layer**: This final layer provides the network’s **output**, such as classifying an image or predicting a value.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"width: 400px; margin: 0 auto;\">\n",
    "   <img src=\"https://raw.githubusercontent.com/danielsimon4/deep-learning-book/refs/heads/main/deeplearningbook/images/neural-network.png\">\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "(1.3)=\n",
    "## 1.3 Neurons\n",
    "\n",
    "In a neural network, each **neuron** is a fundamental unit that takes in **multiple inputs** and processes them to produce a **single output**. As shown above, each neuron in the hidden and output layers connects to all the neurons in the previous layer. These connections have associated values known as **weights** that represents the strength of the connection between the neurons. \n",
    "\n",
    "When an input reaches a neuron, it is multiplied by the weight of its connection, and the results are combined (summed up). An additional value, called a **bias** (b), may be added to adjust the sum. The result may also be passed through an **activation function** (σ), which determines the neuron's output by introducing non-linearity.\n",
    "\n",
    "<div style=\"width: 440px; margin: 0 auto;\">\n",
    "   <img src=\"https://raw.githubusercontent.com/danielsimon4/deep-learning-book/refs/heads/main/deeplearningbook/images/neuron.png\">\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "The output of the above neuron would be:\n",
    "$$\n",
    "\\text{output} = \\sigma\\left(w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b\\right)\n",
    "$$\n",
    "Thus, in general, the output of a neuron can be expressed as:\n",
    "$$\n",
    "\\text{output} = \\sigma\\left(Σ_{j} w_{j} x_{j} + b\\right)\n",
    "$$\n",
    "Using the dot product, the above expression can be expressed as:\n",
    "$$\n",
    "\\text{output} = \\sigma({w} \\cdot {x} + b)\n",
    "$$\n",
    "\n",
    "```{Note}\n",
    "This output becomes the input for neurons in the next layer, and so on, allowing the network to learn complex patterns in the data through layers of transformations.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "(1.4)=\n",
    "## 1.4 How do neural networks learn?\n",
    "\n",
    "Neural networks learn in 3 steps:\n",
    "1. [](1.5)\n",
    "2. Backpropagation\n",
    "3. Update Parameters\n",
    "\n",
    "To better understand their learning process we are goint to create a neural network that predicts if a male baby is healthy or not based on his Age (months),Height (cm), and Weight (kg).\n",
    "\n",
    "| Month | Height (cm) | Weight (kg) | Status    |\n",
    "|-------|-------------|-------------|-----------|\n",
    "| 1     | 53          | 4.0         | Healthy   |\n",
    "| 4     | 62          | 6.0         | Healthy   |\n",
    "| 8     | 71          | 8.5         | Unhealthy |\n",
    "| 3     | 60          | 5.5         | Healthy   |\n",
    "| 9     | 73          | 8.5         | Healthy   |\n",
    "| 2     | 55          | 4.5         | Healthy   |\n",
    "| 12    | 75          | 9.5         | Unhealthy |\n",
    "| 6     | 66          | 7.5         | Unhealthy |\n",
    "| 7     | 69          | 8.0         | Healthy   |\n",
    "| 10    | 74          | 9.0         | Healthy   |\n",
    "\n",
    "\n",
    "The first step is going to be to import the data.\n",
    "\n",
    "```{note}\n",
    "- The `file.read()` method reads the entire content of a file and returns it as a single string.\n",
    "- The `str.splitlines()` method splits a string into a list, where each element corresponds to a line, removing the newline characters.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0000, 53.0000,  4.0000],\n",
       "         [ 4.0000, 62.0000,  6.0000],\n",
       "         [ 8.0000, 71.0000,  8.5000],\n",
       "         [ 3.0000, 60.0000,  5.5000],\n",
       "         [ 9.0000, 73.0000,  8.5000],\n",
       "         [ 2.0000, 55.0000,  4.5000],\n",
       "         [12.0000, 75.0000,  9.5000],\n",
       "         [ 6.0000, 66.0000,  7.5000],\n",
       "         [ 7.0000, 69.0000,  8.0000],\n",
       "         [10.0000, 74.0000,  9.0000]]),\n",
       " tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import PyToch library\n",
    "import torch\n",
    "\n",
    "# load dataset\n",
    "with open('../datasets/babies.csv', 'r') as file:\n",
    "    data = file.read().splitlines()\n",
    "\n",
    "# remove the header\n",
    "data = data[1:]\n",
    "\n",
    "# create lists\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for entry in data:\n",
    "    values = entry.split(',')\n",
    "    age = int(values[0])\n",
    "    height = int(values[1])\n",
    "    weight = float(values[2])\n",
    "    status = 1 if values[3] == 'Healthy' else 0\n",
    "\n",
    "    x.append([age, height, weight])\n",
    "    y.append(status)\n",
    "\n",
    "# convert lists to tensors\n",
    "inputs = torch.tensor(x)\n",
    "labels = torch.tensor(y)\n",
    "\n",
    "# print input and labels tensors\n",
    "inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.3500, 0.3000],\n",
       "        [0.6000, 0.8000, 0.8000],\n",
       "        [0.1000, 0.2500, 0.2000],\n",
       "        [0.7000, 0.9000, 0.8000],\n",
       "        [0.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 1.0000, 1.0000],\n",
       "        [0.4000, 0.5500, 0.6000],\n",
       "        [0.5000, 0.7000, 0.7000],\n",
       "        [0.8000, 0.9500, 0.9000]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize using Min-Max scaling\n",
    "def MinMaxScaling(tensor):\n",
    "    min_vals = tensor.min(dim=0).values\n",
    "    max_vals = tensor.max(dim=0).values\n",
    "    normalized_tensor = (tensor - min_vals) / (max_vals - min_vals)\n",
    "    return normalized_tensor\n",
    "\n",
    "norm_input = MinMaxScaling(inputs)\n",
    "norm_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "(1.5)=\n",
    "## 1.5 Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network will have the following structure:\n",
    "\n",
    "<div style=\"width: 440px; margin: 0 auto;\">\n",
    "   <img src=\"https://raw.githubusercontent.com/danielsimon4/deep-learning-book/refs/heads/main/deeplearningbook/images/babies-nn.png\">\n",
    "</div>\n",
    "\n",
    "For simplicity we are not going to add bias or implement any activation function yet.\n",
    "\n",
    "Please note that using matrix multiplication, we can efficiently evaluate in parallel the outputs of the 6 neurons for the 8 examples at the same time.\n",
    "\n",
    "<br>\n",
    "\n",
    "````{note}\n",
    "`requires_grad=True` tells PyTorch that we are interested in calculating gradients for this leaf tensor (it allows to run *loss.backward()* later).\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6473, -0.3658],\n",
       "        [-1.1477, -0.8890],\n",
       "        [-0.6294, -0.2518],\n",
       "        [-0.8287, -0.9710],\n",
       "        [ 0.0000,  0.0000],\n",
       "        [-0.1798, -1.1398],\n",
       "        [-1.0202, -0.6257],\n",
       "        [-1.1297, -0.7750],\n",
       "        [-0.7552, -1.0497]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intitialize randomly weight matrices\n",
    "w1 = torch.randn((3, 6), requires_grad=True) # (input_to_layer, output_to_layer)\n",
    "w2 = torch.randn((6, 2), requires_grad=True) # (input_to_layer, output_to_layer)\n",
    "\n",
    "# matrix multiplication\n",
    "h1 = norm_input @ w1        # (8,6) = (8,3) x (3,6)\n",
    "h2 = h1 @ w2                # (8,2) = (8,6) x (6,2)\n",
    "h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "(1.6)=\n",
    "## 1.6 Softmax\n",
    "\n",
    "Right now, the neural network is outputing 2 positive and negative numbers for each input example, known as **logits** (or log-counts). \n",
    "\n",
    "We are going to apply **Softmax**, an activation function often used in the output layer of neural networks to **convert logits into probability distributions**. \n",
    "\n",
    "<div style=\"width: 310px; margin: 0 auto;\">\n",
    "    <img src=\"https://images.contentstack.io/v3/assets/bltac01ee6daa3a1e14/blte5e1674e3883fab3/65ef8ba4039fdd4df8335b7c/img_blog_image1_inline_(2).png?width=1024&disable=upscale&auto=webp\">\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Softmax first exponentiates the logits to get only positive numbers, known as **counts**, and then normalizes the counts to get **probability distributions**. We are going to interpret these probabilty distributions as the neural network assignments for how likely each of the 27 characters are to come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4301, 0.5699],\n",
       "        [0.4357, 0.5643],\n",
       "        [0.4067, 0.5933],\n",
       "        [0.5355, 0.4645],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.7231, 0.2769],\n",
       "        [0.4026, 0.5974],\n",
       "        [0.4122, 0.5878],\n",
       "        [0.5731, 0.4269]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = h2.exp() # (5, 27)\n",
    "probs = counts / counts.sum(1, keepdims=True) # (5, 27)\n",
    "probs # (5, 27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "How Neural Networks Work\n",
    "1. **Forward Propagation**: \n",
    "   - Each node receives inputs, applies weights to them (values that determine the importance of each input), sums them up, and passes the sum through an **activation function** (a mathematical function that determines whether and how strongly a node should \"fire\" or pass its signal onward).\n",
    "   \n",
    "\n",
    "\n",
    "2. **Activation Functions**: \n",
    "   - Functions like **ReLU** (Rectified Linear Unit), **sigmoid**, or **tanh** are commonly used in neural networks. They introduce non-linearity, enabling the network to learn complex patterns beyond simple linear relationships.\n",
    "\n",
    "3. **Backpropagation and Training**:\n",
    "   - During training, the network makes predictions and compares them to actual outputs (like labeled data in supervised learning). The **error** (difference between prediction and actual result) is calculated.\n",
    "   - **Backpropagation** is then used to adjust the weights in each layer, gradually minimizing the error through optimization algorithms like **gradient descent**. This process continues over multiple iterations (epochs), allowing the network to improve its accuracy over time.\n",
    "\n",
    "4. **Learning**:\n",
    "   - Neural networks learn by adjusting the weights and biases in the network, effectively tuning themselves to recognize patterns in the data. Over time, the network captures intricate relationships within the data, making it capable of handling complex tasks like language translation or image recognition.\n",
    "\n",
    "### Key Terms\n",
    "- **Weights and Biases**: These are parameters adjusted during training to influence how data is processed through the network.\n",
    "- **Layers and Depth**: More layers (depth) enable the network to recognize complex patterns but also require more computational resources.\n",
    "- **Overfitting and Underfitting**: An overfitted model performs too well on training data but poorly on new data, while an underfitted model doesn’t capture enough patterns in the data. Balancing these is key to creating a robust neural network.\n",
    "\n",
    "### Applications of Neural Networks\n",
    "Neural networks power a variety of AI applications:\n",
    "- **Image Recognition**: Identifying objects, faces, and scenes.\n",
    "- **Natural Language Processing (NLP)**: Language translation, sentiment analysis, and text generation.\n",
    "- **Speech Recognition**: Converting spoken language into text.\n",
    "- **Recommendation Systems**: Suggesting products, music, movies, etc., based on user preferences.\n",
    "\n",
    "In summary, neural networks transform data through layers of interconnected nodes, progressively learning to recognize patterns and make accurate predictions. This powerful architecture forms the basis for many state-of-the-art AI applications.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "At the core of deep learning are **deep neural networks (DNNs)**, which consist of interconnected layers of nodes, or \"neurons.\" These networks are inspired by the human brain’s structure, enabling machines to build hierarchical data representations. For instance, a model might learn to recognize objects in images by first detecting edges, then shapes, and finally objects. Deep learning models are highly effective in tasks traditionally exclusive to human cognition, achieving impressive accuracy in areas such as image and speech recognition, natural language processing (NLP), and even creative tasks like art generation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Deep learning applications are far-reaching, from healthcare, where it aids in diagnostics and drug discovery, to finance, where it improves fraud detection and trading automation. It also drives advancements in autonomous vehicles, language translation, and beyond. Key architectures include **convolutional neural networks (CNNs)** for image analysis, **recurrent neural networks (RNNs)** for sequential data, and **transformers** for NLP and sequence modeling.\n",
    "\n",
    "Despite its rapid advancements, deep learning faces challenges, such as the need for large labeled datasets, high computational resources, and issues with interpretability. However, ongoing research in model optimization, data efficiency, and interpretability is making deep learning an ever-evolving and promising field, continuously pushing the boundaries of what machines can accomplish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an expanded table with additional examples of male babies between 1 and 12 months old, including a mix of healthy and unhealthy status, not in any particular order:\n",
    "\n",
    "| Month | Height (cm) | Weight (kg) | Status    |\n",
    "|-------|-------------|-------------|-----------|\n",
    "| 1     | 53          | 4.0         | Healthy   |\n",
    "| 4     | 62          | 6.0         | Healthy   |\n",
    "| 8     | 71          | 8.5         | Unhealthy |\n",
    "| 3     | 60          | 5.5         | Healthy   |\n",
    "| 9     | 73          | 8.5         | Healthy   |\n",
    "| 2     | 55          | 4.5         | Healthy   |\n",
    "| 12    | 75          | 9.5         | Unhealthy |\n",
    "| 6     | 66          | 7.5         | Unhealthy |\n",
    "| 7     | 69          | 8.0         | Healthy   |\n",
    "| 10    | 74          | 9.0         | Healthy   |\n",
    "| 5     | 64          | 6.5         | Unhealthy |\n",
    "| 11    | 76          | 10.0        | Healthy   |\n",
    "| 3     | 58          | 4.0         | Unhealthy |\n",
    "| 1     | 50          | 3.5         | Unhealthy |\n",
    "| 10    | 73          | 9.5         | Healthy   |\n",
    "| 4     | 59          | 5.0         | Unhealthy |\n",
    "| 8     | 72          | 9.0         | Healthy   |\n",
    "| 9     | 70          | 8.0         | Unhealthy |\n",
    "| 2     | 57          | 4.0         | Healthy   |\n",
    "| 11    | 74          | 9.5         | Unhealthy |\n",
    "| 5     | 67          | 7.0         | Healthy   |\n",
    "| 6     | 65          | 7.0         | Healthy   |\n",
    "\n",
    "This table now contains a greater variety of examples. If you need further modifications or more data, just let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{Note}\n",
    "An **activation function** is a mathematical function that determines whether a node should \"fire\" and how strongly it should pass its signal onward. Some known activation functions are:\n",
    "<div style=\"width: 250px; margin: 0 auto;\">\n",
    "   <img src=\"https://raw.githubusercontent.com/danielsimon4/deep-learning-book/refs/heads/main/deeplearningbook/images/activation-functions.png\">\n",
    "</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
