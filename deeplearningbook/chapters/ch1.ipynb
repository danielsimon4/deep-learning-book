{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; **Artificial Intelligence (AI)** is a branch of computer science focused on creating systems that perform tasks that require human-like intelligence, such as language comprehension, pattern recognition, problem-solving, and decision-making.\n",
    "\n",
    "**Machine Learning (ML)** is a subset of AI that involves training models on data to identify patterns and make predictions. ML models learn from data and improve their accuracy over time using one of these approaches:\n",
    "\n",
    "- **Supervised Learning**: The model is trained on labeled data and learns to associate inputs with outputs, making it well-suited for classification and regression tasks.\n",
    "- **Unsupervised Learning**: The model is trained on unlabeled data and learns to identify hidden patterns or groupings within the data, making it well-suited for clustering and association tasks.\n",
    "- **Reinforcement Learning**: The model learns through rewards and penalties based on its actions, making it well-suited for environments where decision-making is complex, such as strategic games and robotics.\n",
    "\n",
    "**Deep Learning (DL)** is a specialized area within ML that uses neural networks to recognize complex patterns in large datasets.\n",
    "\n",
    "```{figure} ../images/deep-learning.png\n",
    "---\n",
    "width: 140px\n",
    "name: deep-learning\n",
    "---\n",
    "Deep Learning Overview\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; **Neural networks** are computational models inspired by the structure of the human brain, designed to recognize patterns and make predictions. They consist of layers of interconnected nodes (often called neurons) that process information through mathematical operations.\n",
    "\n",
    "A basic neural network has the following structure: \n",
    "- **Input Layer**: The first layer receives raw data, like images, text, or numerical values. Each node in this layer represents an input feature.\n",
    "- **Hidden Layers**: The intermediate layers are between the input and output layers, and process the information. Each hidden layer transforms data from the previous layer, allowing the network to progressively learn and recognize patterns.\n",
    "- **Output Layer**: The final layer provides the network’s output, such as classifying an image or predicting a value.\n",
    "\n",
    "```{figure} ../images/neural-network.png\n",
    "---\n",
    "width: 340px\n",
    "name: neural-network\n",
    "---\n",
    "Basic Structure of a Neural Network\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "(1.3)=\n",
    "## Neurons\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; In a neural network, each **neuron** is a fundamental unit that takes in multiple inputs and processes them to produce a single output. As shown in [*Fig. 2 Basic Structure of a Neural Network*](neural-network), each neuron in the hidden and output layers connects to all the neurons in the previous layer. These connections have associated values, known as **weights**, which are adjusted by the model.\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; During the forward pass, the neuron's inputs ($x_{n}$) are multiplied by their corresponding connection weights ($w_{n}$), and the results are summed. An additional value, the **bias** ($b$), is often added to the weighted sum. The result is typically passed through an **activation function** ($\\sigma$), which introdues non-linearity and normalizes the neuron's output.\n",
    "\n",
    "```{figure} ../images/neuron.png\n",
    "---\n",
    "width: 250px\n",
    "name: neuron\n",
    "---\n",
    "Structure of a Neuron\n",
    "```\n",
    "\n",
    "<p style=\"margin-top: 0;\">The output of the above neuron is:</p>\n",
    "\n",
    "$$\n",
    "\\text{output} = \\sigma(x_{1}w_{1} + x_{2}w_{2} + x_{3}w_{3} + b)\n",
    "$$\n",
    "\n",
    "More generally, the output of a neuron is:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\sigma(x_{1}w_{1} + x_{2}w_{2} + x_{3}w_{3} + \\dots + x_{n}w_{n} + b)\n",
    "$$\n",
    "\n",
    "<p style=\"margin-top: 0;\">where $n$ is the number of inputs to the neuron.</p>\n",
    "\n",
    "The weighted sum can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\sigma(Σ_{n}x_{n}w_{n} + b)\n",
    "$$\n",
    "\n",
    "<p style=\"margin-top: 0;\">where $n$ is the number of inputs to the neuron.</p>\n",
    "\n",
    "```{important}\n",
    "The inputs to a neuron are the outputs of the neurons in the previous layer, and the ouput of a neuron is one of the inputs to the neurons in the next layer.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "The weights represent the strength of connection between the neurons. A larger weight indicates a stronger connection.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## PyTorch\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; We will use **PyTorch** to build our neural networks. PyTorch is an open-source machine learning library widely used for building and training deep learning models due to its flexibility, ease of use, and efficient computation. The following code sets up the environment for working with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch\n",
    "import torch\n",
    "\n",
    "# set print options to avoid scientific notation\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp; &ensp; &ensp; &ensp; PyTorch provides multi-dimensional arrays, known as **tensors**, which are similar to NumPy arrays but optimized for GPU processing. Depending on their dimensions, we will refer to the tensors differently:\n",
    "\n",
    "A 1-dimensional tensor is called a **vector**. A vector with shape (3) would look like this:\n",
    "\n",
    "```python\n",
    "tensor([1, 2, 3])\n",
    "```\n",
    "\n",
    "<p style=\"margin-top: 0;\">A 2-dimensional tensor is called a <strong>matrix</strong>. A matrix with shape (2, 3) would look like this:</p>\n",
    "\n",
    "```python\n",
    "tensor([[1, 2, 3],\n",
    "        [4, 5, 6]])\n",
    "```\n",
    "\n",
    "<p style=\"margin-top: 0;\">A 3 or more dimensional tensor is called an <strong>n-dimensional tensor</strong>. A 3-dimensional tensor with shape (2, 2, 3) would look like this:</p>\n",
    "\n",
    "```python\n",
    "tensor([[[1, 2, 3],\n",
    "         [4, 5, 6]],\n",
    "        [[7, 8, 9],\n",
    "         [10, 11, 12]]])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How Neural Networks Learn\n",
    "\n",
    "Neural networks use supervised learning to fine-tune their parameters. Specifically, they rely on **gradient descent** to adjust their weights and biases during training. This optimization algorithm involves four key steps:\n",
    "\n",
    "1. **Forward Pass:** Process input data through the network to generate predictions.\n",
    "2. **Loss Function:** Measure the error by comparing predictions with actual target values.\n",
    "3. **Backward Pass:** Calculate gradients of the loss function with respect to the network's parameters using backpropagation.\n",
    "4. **Update Parameters:** Adjust weights and biases to reduce the loss, guided by the computed gradients.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; To better understand gradient descent and how neural networks learn, we will create a simple neural network that converts 3-digit binary numbers into their decimal equivalents. The table below shows the equivalences:\n",
    "\n",
    "| Binary | Decimal |\n",
    "|--------|---------|\n",
    "| 000    | 0       |\n",
    "| 001    | 1       |\n",
    "| 010    | 2       |\n",
    "| 011    | 3       |\n",
    "| 100    | 4       |\n",
    "| 101    | 5       |\n",
    "| 110    | 6       |\n",
    "| 111    | 7       |\n",
    "\n",
    "The conversion from a binary number $b_2b_1b_0$ to its decimal equivalent $D$ follows the formula:\n",
    "\n",
    "$$\n",
    "D = b_2 \\cdot 2^2 + b_1 \\cdot 2^1 + b_0 \\cdot 2^0\n",
    "$$\n",
    "\n",
    "where $b_2, b_1, b_0$ are the binary digits 0 or 1.\n",
    "\n",
    "Our neural network won't know this formula. Instead, it will learn to approximate this relationship through training. We will train the model with the following input examples and corresponding targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Targets:\n",
      "tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 1], [0, 1, 1], \n",
    "                  [1, 0, 0], [1, 0, 0], [1, 0, 1], [1, 0, 1], [1, 1, 0], [1, 1, 0], [1, 1, 1], [1, 1, 1]]).float()\n",
    "\n",
    "targets = torch.tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7])\n",
    "\n",
    "print(f\"Input:\\n{x}\\n\")\n",
    "print(f\"Targets:\\n{targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Structure\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; The neural network will have the following structure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To begin, we will initialize the network parameters randomly. For simplicity, we will exclude bias terms and activation functions at this stage.\n",
    "\n",
    "```{note}\n",
    ":class: dropdown\n",
    "The `torch.randn` function generates a tensor filled with random numbers drawn from a standard normal distribution (mean = 0, standard deviation = 1). For more information, please refer to the [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.randn.html).\n",
    "```\n",
    "\n",
    "```{note}\n",
    ":class: dropdown\n",
    "The `requires_grad=True` argument indicates that the tensor should track gradients for operations. This will enable to run `loss.backward()` in the backward pass.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 220\n"
     ]
    }
   ],
   "source": [
    "# seed the random number generator for reproducibility\n",
    "g = torch.Generator().manual_seed(1)\n",
    "\n",
    "# intitialize randomly weight matrices\n",
    "W1 = torch.randn((3, 20), generator=g)   # (input_to_layer, output_from_layer)\n",
    "W2 = torch.randn((20, 8), generator=g)   # (input_to_layer, output_from_layer)\n",
    "\n",
    "# list of parameters\n",
    "parameters = [W1, W2]\n",
    "\n",
    "# track gradients\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# print total number of parameters\n",
    "print(f\"Number of parameters: {sum(p.nelement() for p in parameters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Forward Pass\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; In the **forward pass**, the input data flows through the network, layer by layer, using the formula described before. In section [1.3. Neurons](1.3), we show that the output of a neuron was given by the formula:\n",
    "\n",
    "$$\n",
    "\\small\n",
    "h\n",
    "=\n",
    "\\sigma\\left(\n",
    "\\begin{bmatrix}\n",
    "x_{1} & x_{2} & x_{3} & \\dots & x_{d}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "w_{3} \\\\\n",
    "\\vdots \\\\\n",
    "w_{d}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "b\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "<p style=\"margin-top: 0;\">where:</p>\n",
    "\n",
    "- $d$: Dimensionality of the input vector.\n",
    "\n",
    "<br>\n",
    "\n",
    "The output of a layer is:\n",
    "\n",
    "$$\n",
    "\\small\n",
    "\\begin{bmatrix}\n",
    "h_{1} & h_{2} & h_{3} & \\dots & h_{m}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma\\left(\n",
    "\\begin{bmatrix}\n",
    "x_{1} & x_{2} & x_{3} & \\dots & x_{d}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} & \\dots & w_{1m} \\\\\n",
    "w_{21} & w_{22} & w_{23} & \\dots & w_{2m} \\\\\n",
    "w_{31} & w_{32} & w_{33} & \\dots & w_{3m} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{d1} & w_{d2} & w_{d3} & \\dots & w_{dm} \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{1} & b_{2} & b_{3} & \\dots & b_{m}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "<p style=\"margin-top: 0;\">where:</p>\n",
    "\n",
    "- $d$: Dimensionality of the input vector.\n",
    "- $m$: Number of neurons in the layer.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Matrix multiplication** enables us to efficiently calculate in parallel the output of a layer for several input examples:\n",
    "\n",
    "$$\n",
    "\\small\n",
    "\\begin{bmatrix}\n",
    "h_{11} & h_{12} & h_{13} & \\dots & h_{1m} \\\\\n",
    "h_{21} & h_{22} & h_{23} & \\dots & h_{2m} \\\\\n",
    "h_{31} & h_{32} & h_{33} & \\dots & h_{3m} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "h_{N1} & h_{N2} & h_{N3} & \\dots & h_{Nm}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma\\left(\n",
    "\\begin{bmatrix}\n",
    "x_{11} & h_{12} & h_{13} & \\dots & h_{1d} \\\\\n",
    "x_{21} & h_{22} & h_{23} & \\dots & h_{2d} \\\\\n",
    "x_{31} & h_{32} & h_{33} & \\dots & h_{3d} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{N1} & h_{N2} & h_{N3} & \\dots & h_{Nd}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} & \\dots & w_{1m} \\\\\n",
    "w_{21} & w_{22} & w_{23} & \\dots & w_{2m} \\\\\n",
    "w_{31} & w_{32} & w_{33} & \\dots & w_{3m} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{d1} & w_{d2} & w_{d3} & \\dots & w_{dm} \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{1} & b_{2} & b_{3} & \\dots & b_{k}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "<p style=\"margin-top: 0;\">where:</p>\n",
    "\n",
    "- $d$: Dimensionality of the input vector.\n",
    "- $m$: Number of neurons in the layer.\n",
    "- $N$: Number of examples in a batch.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The outputs of the 20 neurons in the first layer for the 16 examples would be: b\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "h_{1,1} & h_{1,2} & h_{1,3} & \\dots & h_{1,20} \\\\\n",
    "h_{2,1} & h_{2,2} & h_{2,3} & \\dots & h_{2,20} \\\\\n",
    "h_{3,1} & h_{3,2} & h_{3,3} & \\dots & h_{3,20} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "h_{16,1} & h_{16,2} & h_{16,3} & \\dots & h_{16,20}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & x_{1,3} \\\\\n",
    "x_{2,1} & x_{2,2} & x_{2,3} \\\\\n",
    "x_{3,1} & x_{3,2} & x_{3,3} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "x_{16,1} & x_{16,2} & x_{16,3}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & w_{1,3} & \\dots & w_{1,20} \\\\\n",
    "w_{2,1} & w_{2,2} & w_{2,3} & \\dots & w_{2,20} \\\\\n",
    "w_{3,1} & w_{3,2} & w_{3,3} & \\dots & w_{3,20} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\text{where } h_{11} = x_{1}w_{1} + x_{2}w_{2} + x_{3}w_{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication\n",
    "h1 = x @ W1     # (16,20) = (16,3) x (3,20)\n",
    "h2 = h1 @ W2    # (16,8) = (16,20) x (20,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; A **loss function** is a mathematical representation that quantifies how well a machine learning model is performing. It measures the difference between the model's predicted outputs and the actual outputs from the dataset (the **targets**).\n",
    "\n",
    "There are various types of loss functions, each suitable for different tasks:\n",
    "- Regression Tasks: Mean Squared Error (MSE) and Mean Absolute Error (MAE) (both covered in section [](1.8)).\n",
    "- Classification Tasks: Cross-Entropy Loss (covered in [](2.7)) and Hinge Loss.\n",
    "\n",
    "\n",
    "The network's output is compared to the actual values, and a loss function is used to measure the prediction error. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Mean Squared Error\n",
    "\n",
    "The **Mean Squared Error (MSE)** is a commonly used loss function for regression tasks, where the objective is to predict continuous values. MSE computes the average of the squared differences between the predicted values ($\\hat{y}_i$) and actual values($y_i$). The squaring of the errors results in larger penalties for bigger discrepancies, making MSE particularly sensitive to outliers compared to the **Mean Absolute Error (MAE)**, which treats all errors equally.\n",
    "\n",
    "The formulas for MSE and MAE are as follows:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "We will first define the following targets, which are the values to which the neural network's output should converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets:\n",
      "tensor([[ 1.1000, -1.3000],\n",
      "        [ 2.3000,  2.0000],\n",
      "        [-2.5000, -3.2000],\n",
      "        [ 0.2000,  4.3000],\n",
      "        [ 0.6000,  5.3000]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets:\\n{targets}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then calculate the loss using the Mean Squared Error.\n",
    "\n",
    "```{note}\n",
    "The `tensor.item()` method returns the value of a single-element tensor. For more information, please refer to the [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0183\n"
     ]
    }
   ],
   "source": [
    "# calculate loss using MSE\n",
    "loss = ((h2 - targets) ** 2).mean()\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "The network calculates gradients by propagating the error backward through the layers, determining how much each parameter contributed to the error. During **backpropagation**, the neural network computes the **gradient of the loss function** with respect to all its parameters by applying the chain rule of calculus.\n",
    "\n",
    "To compute all the gradients, we will use the PyTorch function `loss.backward()` which kept track of all the operations during the forwards pass. In  [](ch3.ipynb) we will perform a backward pass manually to better understand how backpropagation works and how the gradients are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the gradients to None\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "# calculate gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "\n",
    "The calculated gradients are used to adjust the weights and biases through an optimization algorithm to minimize the loss function. Once the gradients have been computed, the neural network progresively **updates its parameters** trying minimize the loss function. The most common optimization techniques for updating the parameters are **gradient descent**, **stochastic gradient descent (SGD)**, and **Adam**.\n",
    "\n",
    "A **gradient** is a vector that represents the rate of change of a function with respect to its input variables. By default, gradients point in the **direction of steepest ascent**, that is, the direction in which the function increases the fastest. Since we want to minimize the loss function, we will move in the opposite direction of the gradients. \n",
    "\n",
    "\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "**Gradient descent** is an optimization algorithm used to minimize a function, frequently used in neural networks training to minimize the loss function.\n",
    "\n",
    "Gradient descent updates each parameter by subtracting a fraction of the gradient from its current value, scaled by a factor known as the **learning rate**. The learning rate determines the size of the steps we take towards the minimum. A smaller learning rate results in more precise but slower convergence, while a larger learning rate can speed up convergence but may risk overshooting the minimum. The parameter update rule for gradient descent can be expressed mathematically as:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\nabla L(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ represents the parameters of the neural network\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\nabla L(\\theta)$ is the gradient of the loss function with respect to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 before GD:\n",
      "tensor([[-1.5256, -0.7502,  0.6995,  0.1991,  0.8657,  0.2444],\n",
      "        [-0.6629,  0.8073,  0.4391,  1.1712, -2.2456, -1.4465],\n",
      "        [ 0.0612, -0.6177, -0.7981, -0.1316, -0.7984,  0.3357]])\n",
      "W1 gradients:\n",
      "tensor([[ 0.0536, -0.0771,  0.2550,  0.1593,  0.0444,  0.2188],\n",
      "        [-0.0216,  0.0368, -0.0499, -0.0180, -0.0137, -0.0411],\n",
      "        [ 0.0185, -0.0187,  0.1620,  0.1197,  0.0213,  0.1415]])\n",
      "W1 after GD:\n",
      "tensor([[-1.5310, -0.7425,  0.6740,  0.1832,  0.8613,  0.2225],\n",
      "        [-0.6608,  0.8036,  0.4441,  1.1730, -2.2442, -1.4423],\n",
      "        [ 0.0593, -0.6159, -0.8143, -0.1436, -0.8006,  0.3216]])\n",
      "\n",
      "W2 before GD:\n",
      "tensor([[ 0.3935,  1.1322],\n",
      "        [-0.5404, -2.2102],\n",
      "        [ 2.1130, -0.0040],\n",
      "        [ 1.3800, -1.3505],\n",
      "        [ 0.3455,  0.5046],\n",
      "        [ 1.8213, -0.1814]])\n",
      "W2 gradients:\n",
      "tensor([[-0.1637, -0.0016],\n",
      "        [-0.1570, -0.0064],\n",
      "        [ 0.0129,  0.0072],\n",
      "        [-0.0138, -0.0103],\n",
      "        [ 0.0964,  0.0373],\n",
      "        [ 0.0894,  0.0136]])\n",
      "W2 after GD2:\n",
      "tensor([[ 0.4099,  1.1324],\n",
      "        [-0.5247, -2.2096],\n",
      "        [ 2.1117, -0.0047],\n",
      "        [ 1.3814, -1.3495],\n",
      "        [ 0.3358,  0.5009],\n",
      "        [ 1.8124, -0.1828]])\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1 # learning rate\n",
    "\n",
    "# clone for later comparison \n",
    "old_W1 = W1.data.clone()\n",
    "old_W2 = W2.data.clone()\n",
    "\n",
    "# update parameters using gradient descent\n",
    "for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "print(f\"W1 before GD:\\n{old_W1}\")\n",
    "print(f\"W1 gradients:\\n{W1.grad}\")\n",
    "print(f\"W1 after GD:\\n{W1.data}\\n\")\n",
    "\n",
    "print(f\"W2 before GD:\\n{old_W2}\")\n",
    "print(f\"W2 gradients:\\n{W2.grad}\")\n",
    "print(f\"W2 after GD2:\\n{W2.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```{note}\n",
    "Please note that the gradients indicate whether the values should be increased or decreased, as well as the magnitude of the adjustment.\n",
    "```\n",
    "\n",
    "\n",
    "## Neural Network Training\n",
    "\n",
    "During **training**, a neural network iteratively makes a forward pass, calcules the loss, makes a backward pass and updates its parameters. Each iteration is usually called a **training step**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 30\n"
     ]
    }
   ],
   "source": [
    "# intitialize randomly weight matrices\n",
    "g = torch.Generator().manual_seed(1)\n",
    "W1 = torch.randn((3, 6), generator=g)   # (input_to_layer, output_from_layer)\n",
    "W2 = torch.randn((6, 2), generator=g)   # (input_to_layer, output_from_layer)\n",
    "\n",
    "\n",
    "# parameters\n",
    "parameters = [W1, W2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(f\"Number of parameters: {sum(p.nelement() for p in parameters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0/15     Loss: 0.0183\n",
      "Step:  1/15     Loss: 0.0062\n",
      "Step:  2/15     Loss: 0.0054\n",
      "Step:  3/15     Loss: 0.0051\n",
      "Step:  4/15     Loss: 0.0049\n",
      "Step:  5/15     Loss: 0.0047\n",
      "Step:  6/15     Loss: 0.0045\n",
      "Step:  7/15     Loss: 0.0044\n",
      "Step:  8/15     Loss: 0.0043\n",
      "Step:  9/15     Loss: 0.0042\n",
      "Step: 10/15     Loss: 0.0041\n",
      "Step: 11/15     Loss: 0.0040\n",
      "Step: 12/15     Loss: 0.0040\n",
      "Step: 13/15     Loss: 0.0039\n",
      "Step: 14/15     Loss: 0.0039\n"
     ]
    }
   ],
   "source": [
    "max_steps = 15      # train iterations\n",
    "lr = 0.1            # learning rate\n",
    "\n",
    "# list to keep track of the loss in each step\n",
    "track_loss = []\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # forward pass\n",
    "    h1 = x @ W1     # (5,6) = (5,3) x (3,6)\n",
    "    h2 = h1 @ W2    # (5,2) = (5,6) x (6,2)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = ((h2 - targets) ** 2).mean()\n",
    "    track_loss.append(loss.item())\n",
    "    print(f\"Step: {step:2d}/{max_steps}     Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(track_loss)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m);\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(track_loss)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, through gradient descent, the neural network gradually reduced its loss and improved its predictions over time. Comparing the initial output to the final output, we can observe that the final predictions are closer to the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First output:\n",
      "tensor([[ 1.0329, -1.2264],\n",
      "        [ 2.3319,  2.0953],\n",
      "        [-2.8955, -3.1665],\n",
      "        [ 0.1669,  4.3554],\n",
      "        [ 0.5632,  5.2940]])\n",
      "Last output:\n",
      "tensor([[ 0.9563, -1.2406],\n",
      "        [ 2.2978,  2.0103],\n",
      "        [-2.5604, -3.1827],\n",
      "        [ 0.2316,  4.3254],\n",
      "        [ 0.5060,  5.2981]], grad_fn=<MmBackward0>)\n",
      "Targets:\n",
      "tensor([[ 1.1000, -1.3000],\n",
      "        [ 2.3000,  2.0000],\n",
      "        [-2.5000, -3.2000],\n",
      "        [ 0.2000,  4.3000],\n",
      "        [ 0.6000,  5.3000]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial output:\\n{old_h2}\")\n",
    "print(f\"Final output:\\n{h2}\")\n",
    "print(f\"Targets:\\n{targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
