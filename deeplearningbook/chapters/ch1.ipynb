{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Neural Networks\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.1)=\n",
    "## 1.1 Deep Learning\n",
    "\n",
    "**Artificial Intelligence (AI)** is a branch of computer science focused on creating systems that perform tasks requiring **human-like intelligence**, such as language comprehension, pattern recognition, problem-solving, and decision-making. AI aims to enable machines to perform complex tasks in ways that mimic human reasoning and adaptability.\n",
    "\n",
    "**Machine Learning (ML)** is a subset of AI that involves **training algorithms on data** to identify patterns and make predictions. ML models learn from data and improve their accuracy over time, typically using one of three main approaches:\n",
    "- **Supervised Learning**: The model is trained on labeled data, where each input is paired with a known output. The model learns to associate inputs with outputs, making it well-suited for tasks such as classification (e.g., image recognition) and regression (e.g., predicting prices).\n",
    "- **Unsupervised Learning**: The model is trained on unlabeled data, without predefined outputs. This approach is used to discover hidden patterns or groupings within the data, commonly applied in clustering and association tasks.\n",
    "- **Reinforcement Learning**: The model learns through feedback from rewards and penalties based on its actions. Reinforcement learning is often applied in environments where decision-making is complex, such as strategic games (e.g., chess) and robotics, where learning occurs via trial and error.\n",
    "\n",
    "**Deep Learning (DL)** is a specialized area within ML that uses **neural networks** to recognize complex patterns in large datasets.\n",
    "\n",
    "```{figure} ../images/deep-learning.png\n",
    "---\n",
    "width: 140px\n",
    "name: deep-learning\n",
    "---\n",
    "Deep Learning Overview\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.2)=\n",
    "## 1.2 Neural Networks\n",
    "\n",
    "**Neural networks** are computational models inspired by the structure of the human brain, designed to recognize patterns and make predictions. They consist of **layers of interconnected nodes** (often called neurons) that process information through mathematical operations. \n",
    "\n",
    "A basic neural network has the following structure: \n",
    "- **Input Layer**: This first layer receives raw data, like images, text, or numerical values. Each node in this layer represents an **input feature**.\n",
    "- **Hidden Layers**: These intermediate layers between the input and output layers **process information**. Each hidden layer transforms data from the previous layer, allowing the network to progressively learn and recognize patterns.\n",
    "- **Output Layer**: This final layer provides the network’s **output**, such as classifying an image or predicting a value.\n",
    "\n",
    "```{figure} ../images/neural-network.png\n",
    "---\n",
    "width: 340px\n",
    "name: neural-network\n",
    "---\n",
    "Basic Structure of a Neural Network\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "(1.3)=\n",
    "## 1.3 Neurons\n",
    "\n",
    "In a neural network, each **neuron** is a fundamental unit that takes in **multiple inputs** and processes them to produce a **single output**. As shown in [*Fig. 2 Basic Structure of a Neural Network*](neural-network), each neuron in the hidden and output layers connects to all the neurons in the previous layer. These connections have associated values known as **weights** that represent the strength of the connection between the neurons. \n",
    "\n",
    "When an input reaches a neuron, it is multiplied by the weight of its connection, and the results are combined (summed up). An additional value, called a **bias** (b), may be added to adjust the sum. The result may also be passed through an **activation function** (σ), which determines the neuron's output by introducing non-linearity.\n",
    "\n",
    "```{figure} ../images/neuron.png\n",
    "---\n",
    "width: 250px\n",
    "name: neuron\n",
    "---\n",
    "Structure of a Neuron\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "The output of the above neuron would be:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\sigma\\left(w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b\\right)\n",
    "$$\n",
    "\n",
    "Thus, in general, the output of a neuron can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\sigma\\left(Σ_{j} w_{j} x_{j} + b\\right)\n",
    "$$\n",
    "\n",
    "Using matrix multiplication, the above expression can be represented as:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\sigma({w} \\cdot {x} + b)\n",
    "$$\n",
    "\n",
    "```{important}\n",
    "The output of the above neuron would be one of the inputs for the neurons in the next layer, and so on, allowing the neural network to learn complex patterns in the data through layers of transformations.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "Plso note that a larger weight indicates a stronger connection between the neurons, while the bias term allows the activation of a neuron to be adjusted independently of its inputs, enabling the model to better fit the training data and capture more complex patterns.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.4)=\n",
    "## 1.4 PyTorch\n",
    "\n",
    "We will use **PyTorch** to build our neural networks. PyTorch is an open-source machine learning library widely used for building and training deep learning models due to its flexibility, ease of use, and efficient computation. It provides multi-dimensional arrays, known as **tensors**, which are similar to NumPy arrays but optimized for GPU processing. Depending on their dimensions, we will refer to the tensors differently.\n",
    "\n",
    "A 1-dimensional tensor is called a **vector**. A vector with shape (3) would look like this:\n",
    "\n",
    "```python\n",
    "tensor([1, 2, 3])\n",
    "```\n",
    "\n",
    "A 2-dimensional tensor is called a **matrix**: A matrix with shape (2, 3) would look like this:\n",
    "\n",
    "```python\n",
    "tensor([[1, 2, 3],\n",
    "        [4, 5, 6]])\n",
    "```\n",
    "\n",
    "A tensor with 3 or more dimensions is called an **n-dimensional tensor**. A 3D tensor with shape (2, 2, 3) would look like this:\n",
    "\n",
    "```python\n",
    "tensor([[[1, 2, 3],\n",
    "         [4, 5, 6]],\n",
    "        [[7, 8, 9],\n",
    "         [10, 11, 12]]])\n",
    "```\n",
    "\n",
    "The following code will set up the environment for working with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch library\n",
    "import torch\n",
    "\n",
    "# seed the random number generator for reproducibility\n",
    "g = torch.Generator().manual_seed(2)\n",
    "\n",
    "# set print options to avoid scientific notation\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "(1.5)=\n",
    "## 1.5 Understanding How Neural Networks Learn\n",
    "\n",
    "Neural networks use supervised learning to **fine-tune their parameters**. In other words, neural network learn by adjusting thier weights and biases by iteratively performing four key steps:\n",
    "- **Forward pass**: The input data flows through the network layer by layer, producing an output.\n",
    "- **Loss Function**: The network's output is compared to the true target, and a loss function is used to measure the prediction error.\n",
    "- **Backpropagation**: The network calculates gradients by propagating the error backward through the layers, determining how much each parameter contributed to the error.\n",
    "- **Update Parameters**: The calculated gradients are used to adjust the weights and biases through an optimization algorithm (like gradient descent or Adam), updating the parameters to minimize the loss.\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.6)=\n",
    "## 1.6 Forward Pass\n",
    "\n",
    "We are going to implement a **forward pass** using 5 input examples for the neural network presented in [*Fig. 2 Basic Structure of a Neural Network*](neural-network). This network had 3 input features, 2 hidden layers with 6 neurons each, and 2 output neurons. For simplicity, we are not going to add any bias or activation functions yet.\n",
    "\n",
    "```{note}\n",
    "- The `torch.randn` function generates a tensor filled with random numbers drawn from a standard normal distribution (mean = 0, standard deviation = 1).\n",
    "- The `requires_grad=True` argument indicates that the tensor should track gradients for operations. This enables to run `loss.backward()` during backpropagation so that PyToch computes all the gradients of the loss function with respect to each parameter in the model.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[-2.574, -0.343, -1.281],\n",
      "        [-0.447,  0.061,  1.326],\n",
      "        [-1.069, -0.536, -1.345],\n",
      "        [-0.833,  0.910,  1.061],\n",
      "        [-0.582,  0.698, -1.133]])\n",
      "\n",
      "Output:\n",
      "tensor([[10.319,  5.496],\n",
      "        [ 8.038, -0.801],\n",
      "        [ 1.571,  3.432],\n",
      "        [ 7.749, -0.519],\n",
      "        [-2.501,  1.623]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# intitialize randomly 5 input examples\n",
    "x = torch.randn((5, 3), generator=g)                  # (num_examples, input_features)\n",
    "\n",
    "# intitialize randomly weight matrices\n",
    "w1 = torch.randn((3, 6), generator=g, requires_grad=True) # (input_to_layer, output_from_layer)\n",
    "w2 = torch.randn((6, 2), generator=g, requires_grad=True) # (input_to_layer, output_from_layer)\n",
    "\n",
    "# matrix multiplication\n",
    "h1 = x @ w1     # (5,6) = (5,3) x (3,6)\n",
    "h2 = h1 @ w2    # (5,2) = (5,6) x (6,2)\n",
    "\n",
    "# print input and output matrices\n",
    "print(f\"Input:\\n{x}\\n\")\n",
    "print(f\"Output:\\n{h2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{important}\n",
    "Please note that using matrix multiplication, we can efficiently **evaluate in parallel** the outputs for the 5 input examples at the same time.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the neural network is outputing a pair of positive and negative values for each input example. To transform these outputs into a probability distribution, we will apply the **Softmax** function, which ensure that the output for each example sums to one, representing the likelihood of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0.277,     0.723],\n",
       "        [    0.711,     0.289],\n",
       "        [    0.963,     0.037],\n",
       "        [    0.998,     0.002],\n",
       "        [    0.000,     1.000]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.softmax(h2, dim=1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indices of the correct class for each example\n",
    "targets = torch.tensor([1, 0, 1, 0, 1])\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A **loss function** is a mathematical representation that quantifies how well a machine learning model's predictions match the actual target values. It measures the discrepancy between the predicted outputs and the true outputs in a dataset. The goal of training a model is to minimize this loss function, which helps improve the model's accuracy and performance.\n",
    "\n",
    "\n",
    "Purpose: The loss function provides feedback on the model's performance during training. It helps in guiding the optimization process by indicating how much the model's predictions deviate from the true values.\n",
    "\n",
    "Types: There are various types of loss functions, each suitable for different tasks:\n",
    "\n",
    "Regression Tasks: For predicting continuous values, common loss functions include Mean Squared Error (MSE) and Mean Absolute Error (MAE).\n",
    "Classification Tasks: For predicting discrete classes, loss functions like Cross-Entropy Loss or Hinge Loss are often used.\n",
    "Optimization: During training, algorithms like gradient descent use the loss function to update the model's parameters (weights and biases) in order to minimize the loss. The gradients of the loss function with respect to the model parameters are calculated, indicating how to adjust each parameter to reduce the loss.\n",
    "\n",
    "\n",
    "\n",
    "```{note}\n",
    "The [`tensor.item()`](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html) method returns the value of the single-element tensor.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7931\n"
     ]
    }
   ],
   "source": [
    "loss = -torch.log(probs[range(probs.shape[0]), targets]).mean()\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
