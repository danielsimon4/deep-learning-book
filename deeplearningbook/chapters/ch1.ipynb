{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Nueral Networks\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.1)=\n",
    "## Deep Learning\n",
    "\n",
    "**Artificial Intelligence (AI)** is a branch of computer science focused on creating systems that perform tasks requiring **human-like intelligence**, such as language comprehension, pattern recognition, problem-solving, and decision-making. AI aims to enable machines to perform complex tasks in ways that mimic human reasoning and adaptability.\n",
    "\n",
    "**Machine Learning (ML)** is a subset of AI that involves **training algorithms on data** to identify patterns and make predictions. ML models learn from data and improve their accuracy over time, typically using one of three main approaches:\n",
    "- **Supervised Learning**: The model is trained on labeled data, where each input is paired with a known output. The model learns to associate inputs with outputs, making it well-suited for tasks such as classification (e.g., image recognition) and regression (e.g., predicting prices).\n",
    "- **Unsupervised Learning**: The model is trained on unlabeled data, without predefined outputs. This approach is used to discover hidden patterns or groupings within the data, commonly applied in clustering and association tasks.\n",
    "- **Reinforcement Learning**: The model learns through feedback from rewards and penalties based on its actions. Reinforcement learning is often applied in environments where decision-making is complex, such as strategic games (e.g., chess) and robotics, where learning occurs via trial and error.\n",
    "\n",
    "**Deep Learning (DL)** is a specialized area within ML that uses **neural networks** to recognize complex patterns in large datasets.\n",
    "\n",
    "<div style=\"width: 410px; margin: 0 auto;\">\n",
    "   <img src=\"https://raw.githubusercontent.com/danielsimon4/deep-learning-book/refs/heads/main/deeplearningbook/images/deep-learning.png\">\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.2)=\n",
    "## 1.2 Neural Networks\n",
    "\n",
    "**Neural networks** are computational models inspired by the structure of the human brain, designed to recognize patterns and make predictions. They consist of **layers of interconnected nodes** (often called neurons) that process information through mathematical operations. \n",
    "\n",
    "A basic neural network has the following structure: \n",
    "- **Input Layer**: This first layer receives raw data, like images, text, or numerical values. Each node in this layer represents an **input feature**.\n",
    "- **Hidden Layers**: These intermediate layers between the input and output layers **process information**. Each hidden layer transforms data from the previous layer, allowing the network to progressively learn and recognize patterns.\n",
    "- **Output Layer**: This final layer provides the network’s **output**, such as classifying an image or predicting a value.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"width: 350px; margin: 0 auto;\">\n",
    "   <img src=\"https://raw.githubusercontent.com/danielsimon4/deep-learning-book/refs/heads/main/deeplearningbook/images/neural-network.png\">\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "(1.3)=\n",
    "## 1.3 Neurons\n",
    "\n",
    "In a neural network, each **neuron** is a fundamental unit that takes in **multiple inputs** and processes them to produce a **single output**. As shown above, each neuron in the hidden and output layers connects to all the neurons in the previous layer. These connections have associated values known as **weights** that represent the strength of the connection between the neurons. \n",
    "\n",
    "When an input reaches a neuron, it is multiplied by the weight of its connection, and the results are combined (summed up). An additional value, called a **bias** (b), may be added to adjust the sum. The result may also be passed through an **activation function** (σ), which determines the neuron's output by introducing non-linearity.\n",
    "\n",
    "<div style=\"width: 280px; margin: 0 auto;\">\n",
    "   <img src=\"https://raw.githubusercontent.com/danielsimon4/deep-learning-book/refs/heads/main/deeplearningbook/images/neuron.png\">\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "The output of the above neuron would be:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\sigma\\left(w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b\\right)\n",
    "$$\n",
    "\n",
    "Thus, in general, the output of a neuron can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\sigma\\left(Σ_{j} w_{j} x_{j} + b\\right)\n",
    "$$\n",
    "\n",
    "Using matrix multiplication, the above expression can be represented as:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\sigma({w} \\cdot {x} + b)\n",
    "$$\n",
    "\n",
    "```{important}\n",
    "Please note that this output would be an input for the neurons in the next layer, and so on, allowing the network to learn complex patterns in the data through layers of transformations.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.4)=\n",
    "## 1.4 PyToch\n",
    "\n",
    "We will use **Pytorch** to build our neural networks. PyTorch is an open-source machine learning library widely used for building and training deep learning models due to its flexibility, ease of use, and efficient computation. It provides multi-dimensional arrays, known as **tensors**, which are similar to NumPy arrays but optimized for GPU processing. Depending on their dimensions, we will refer to the tensors differently.\n",
    "\n",
    "A 1-dimensional tensor is called a **vector**. A vector with shape (3) would look like this:\n",
    "```python\n",
    "[1, 2, 3]\n",
    "```\n",
    "A 2-dimensional tensor is called a **matrix**: A matrix with shape (2,3) would look like this:\n",
    "```python\n",
    "[[1, 2, 3],\n",
    "[4, 5, 6]]\n",
    "```\n",
    "A tensor with 3 or more dimensions is called an **n-dimensional tensor**. A 3D tensor with shape (2, 2, 3) would look like this:\n",
    "```python\n",
    "[[[1, 2, 3],\n",
    "[4, 5, 6]],\n",
    "[[7, 8, 9],\n",
    "[10, 11, 12]]]\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.5)=\n",
    "## 1.5 Understanding How Neural Networks Learn\n",
    "\n",
    "Neural networks use supervised learning to **fine-tune their parameters**. In other words, nueral network learn by adjusting thier weights and biases by iteratively performing four key steps:\n",
    "- [](1.4.1): The input data flows through the network layer by layer, producing an output.\n",
    "- [](1.4.2): The network's output is compared to the true target, and a loss function is used to measure the prediction error.\n",
    "- [](1.4.3): The network calculates gradients by propagating the error backward through the layers, determining how much each parameter contributed to the error.\n",
    "- [](1.4.4): The calculated gradients are used to adjust the weights and biases through an optimization algorithm (e.g., gradient descent), updating the parameters to minimize the loss.\n",
    "\n",
    "<br>\n",
    "\n",
    "(1.5)=\n",
    "## 1.5 Forward Pass\n",
    "\n",
    "We are going to implement a **forward pass** of the neural network presented above, which had 3 input features, 2 6-neurons hidden layers, and 2 output neurons. For simplicity, we are not going to add any bias and activation functions yet.\n",
    "\n",
    "\n",
    "\n",
    "```{note}\n",
    "`requires_grad=True` tells PyTorch that we are interested in calculating gradients for this tensor (it allows to run `loss.backward()` later).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-13.5011,   0.9836],\n",
       "        [  1.7564,  -6.0143],\n",
       "        [  7.4770,   0.6555],\n",
       "        [  7.6317,  -7.5269],\n",
       "        [ -0.8718,  -0.7586]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import PyTorch library\n",
    "import torch\n",
    "\n",
    "# intitialize randomly 5 input examples\n",
    "x = torch.randn((5, 3))                      # (num_examples, input_features)\n",
    "\n",
    "# intitialize randomly weight matrices\n",
    "w1 = torch.randn((3, 6), requires_grad=True) # (input_to_layer, output_to_layer)\n",
    "w2 = torch.randn((6, 2), requires_grad=True) # (input_to_layer, output_to_layer)\n",
    "\n",
    "# matrix multiplication\n",
    "h1 = x @ w1         # (5,6) = (5,3) x (3,6)\n",
    "output = h1 @ w2    # (5,2) = (5,6) x (6,2)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{important}\n",
    "Please note that using matrix multiplication, we can efficiently **evaluate in parallel** the outputs for the 5 examples at the same time.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "(1.4.2)=\n",
    "### 1.4.2 Calculate loss\n",
    "\n",
    "During evaluation, we used the average negative log likelihood to assess the quality of the generated words. During training, we will use it as a **loss function** to **evaluate the setting of the parameters** of the neural network.\n",
    "\n",
    "To compute the average negative log likelihood, we are now going to use the individual probabilities assigned by the neural network to the actual correct next characters. In this case,the individual probabilites would be:\n",
    "\n",
    "*probs[0, 5], probs[1, 13], probs[2, 13], probs[3, 1], and probs[4, 0]*\n",
    "\n",
    "We can retrieve these probabilities by indexing into the probability distributions at the indices corresponding to the correct labels.\n",
    "\n",
    "```{Note}\n",
    "The `torch.arange(n)*` function returns a one-dimensional tensor containing values [0, 1, 2, .., n-1]. [PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.arange.html).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 9 is out of bounds for dimension 0 with size 9",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mlog()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mIndexError\u001b[0m: index 9 is out of bounds for dimension 0 with size 9"
     ]
    }
   ],
   "source": [
    "loss = -probs[torch.arange(10), labels].log().mean()\n",
    "print('Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "How Neural Networks Work\n",
    "1. **Forward Propagation**: \n",
    "   - Each node receives inputs, applies weights to them (values that determine the importance of each input), sums them up, and passes the sum through an **activation function** (a mathematical function that determines whether and how strongly a node should \"fire\" or pass its signal onward).\n",
    "   \n",
    "\n",
    "\n",
    "2. **Activation Functions**: \n",
    "   - Functions like **ReLU** (Rectified Linear Unit), **sigmoid**, or **tanh** are commonly used in neural networks. They introduce non-linearity, enabling the network to learn complex patterns beyond simple linear relationships.\n",
    "\n",
    "3. **Backpropagation and Training**:\n",
    "   - During training, the network makes predictions and compares them to actual outputs (like labeled data in supervised learning). The **error** (difference between prediction and actual result) is calculated.\n",
    "   - **Backpropagation** is then used to adjust the weights in each layer, gradually minimizing the error through optimization algorithms like **gradient descent**. This process continues over multiple iterations (epochs), allowing the network to improve its accuracy over time.\n",
    "\n",
    "### Key Terms\n",
    "- **Weights and Biases**: These are parameters adjusted during training to influence how data is processed through the network.\n",
    "- **Layers and Depth**: More layers (depth) enable the network to recognize complex patterns but also require more computational resources.\n",
    "- **Overfitting and Underfitting**: An overfitted model performs too well on training data but poorly on new data, while an underfitted model doesn’t capture enough patterns in the data. Balancing these is key to creating a robust neural network.\n",
    "\n",
    "### Applications of Neural Networks\n",
    "Neural networks power a variety of AI applications:\n",
    "- **Image Recognition**: Identifying objects, faces, and scenes.\n",
    "- **Natural Language Processing (NLP)**: Language translation, sentiment analysis, and text generation.\n",
    "- **Speech Recognition**: Converting spoken language into text.\n",
    "- **Recommendation Systems**: Suggesting products, music, movies, etc., based on user preferences.\n",
    "\n",
    "In summary, neural networks transform data through layers of interconnected nodes, progressively learning to recognize patterns and make accurate predictions. This powerful architecture forms the basis for many state-of-the-art AI applications.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "At the core of deep learning are **deep neural networks (DNNs)**, which consist of interconnected layers of nodes, or \"neurons.\" These networks are inspired by the human brain’s structure, enabling machines to build hierarchical data representations. For instance, a model might learn to recognize objects in images by first detecting edges, then shapes, and finally objects. Deep learning models are highly effective in tasks traditionally exclusive to human cognition, achieving impressive accuracy in areas such as image and speech recognition, natural language processing (NLP), and even creative tasks like art generation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Deep learning applications are far-reaching, from healthcare, where it aids in diagnostics and drug discovery, to finance, where it improves fraud detection and trading automation. It also drives advancements in autonomous vehicles, language translation, and beyond. Key architectures include **convolutional neural networks (CNNs)** for image analysis, **recurrent neural networks (RNNs)** for sequential data, and **transformers** for NLP and sequence modeling.\n",
    "\n",
    "Despite its rapid advancements, deep learning faces challenges, such as the need for large labeled datasets, high computational resources, and issues with interpretability. However, ongoing research in model optimization, data efficiency, and interpretability is making deep learning an ever-evolving and promising field, continuously pushing the boundaries of what machines can accomplish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an expanded table with additional examples of male babies between 1 and 12 months old, including a mix of healthy and unhealthy status, not in any particular order:\n",
    "\n",
    "| Month | Height (cm) | Weight (kg) | Status    |\n",
    "|-------|-------------|-------------|-----------|\n",
    "| 1     | 53          | 4.0         | Healthy   |\n",
    "| 4     | 62          | 6.0         | Healthy   |\n",
    "| 8     | 71          | 8.5         | Unhealthy |\n",
    "| 3     | 60          | 5.5         | Healthy   |\n",
    "| 9     | 73          | 8.5         | Healthy   |\n",
    "| 2     | 55          | 4.5         | Healthy   |\n",
    "| 12    | 75          | 9.5         | Unhealthy |\n",
    "| 6     | 66          | 7.5         | Unhealthy |\n",
    "| 7     | 69          | 8.0         | Healthy   |\n",
    "| 10    | 74          | 9.0         | Healthy   |\n",
    "| 5     | 64          | 6.5         | Unhealthy |\n",
    "| 11    | 76          | 10.0        | Healthy   |\n",
    "| 3     | 58          | 4.0         | Unhealthy |\n",
    "| 1     | 50          | 3.5         | Unhealthy |\n",
    "| 10    | 73          | 9.5         | Healthy   |\n",
    "| 4     | 59          | 5.0         | Unhealthy |\n",
    "| 8     | 72          | 9.0         | Healthy   |\n",
    "| 9     | 70          | 8.0         | Unhealthy |\n",
    "| 2     | 57          | 4.0         | Healthy   |\n",
    "| 11    | 74          | 9.5         | Unhealthy |\n",
    "| 5     | 67          | 7.0         | Healthy   |\n",
    "| 6     | 65          | 7.0         | Healthy   |\n",
    "\n",
    "This table now contains a greater variety of examples. If you need further modifications or more data, just let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{Note}\n",
    "An **activation function** is a mathematical function that determines whether a node should \"fire\" and how strongly it should pass its signal onward. Some known activation functions are:\n",
    "<div style=\"width: 250px; margin: 0 auto;\">\n",
    "   <img src=\"https://raw.githubusercontent.com/danielsimon4/deep-learning-book/refs/heads/main/deeplearningbook/images/activation-functions.png\">\n",
    "</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
