{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; **Neural networks** are computational models inspired by the structure of the human brain, designed to recognize patterns and make predictions. They consist of layers of interconnected nodes (often called neurons) that process information through mathematical operations.\n",
    "\n",
    "A basic neural network has the following structure:\n",
    "\n",
    "1. **Input Layer**: The first layer receives raw data, like images, text, or numerical values. Each node in this layer represents an input dimension.\n",
    "2. **Hidden Layers**: The intermediate layers are between the input and output layers, and process the information. Each hidden layer transforms data from the previous layer, allowing the network to progressively learn and recognize patterns.\n",
    "3. **Output Layer**: The final layer provides the networkâ€™s output, such as classifying an image or predicting a value.\n",
    "\n",
    "```{figure} ../images/neural-network.png\n",
    "---\n",
    "width: 340px\n",
    "name: neural-network\n",
    "---\n",
    "Basic Structure of a Neural Network\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "(1.3)=\n",
    "## Neurons\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; A **neuron** is a fundamental unit that takes in multiple inputs and processes them to produce a single output. As shown above in [*Fig. 1.2*](neural-network), each neuron in the hidden and output layers connects to all the neurons in the previous layer. These connections have associated values, known as **weights**, which are adjusted by the model. The weights represent the strength of connection between the neurons. A greater weight indicates a stronger connection.\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; To calculate the output of a neuron, all the inputs to the neuron ($x_{1}, x_{2}, \\dots, x_{d}$) are multiplied by their corresponding connection weights ($w_{1}, w_{2}, \\dots, w_{d}$), and the products are summed. A numerical value, called the **bias** ($b$), is then added to the weighted sum. Finally, the result is passed through an **activation function** ($\\sigma$) that returns the output of the neuron, often called the neuron's **activation value** ($h$).\n",
    "\n",
    "```{figure} ../images/neuron.png\n",
    "---\n",
    "width: 250px\n",
    "name: neuron\n",
    "---\n",
    "Structure of a Neuron\n",
    "```\n",
    "\n",
    "```{admonition} Neuron's output\n",
    "<p class=\"bottom-margin\">The activation value of a neuron is given by the formula:</p>\n",
    "\n",
    "$$\n",
    "\\small\n",
    "h = \\sigma(x_{1}w_{1} + x_{2}w_{2} + \\dots + x_{d}w_{d} + b)\n",
    "$$\n",
    "\n",
    "<p class=\"no-top-margin\">where:</p>\n",
    "\n",
    "- $d$: Dimensionality of the input vector.\n",
    "```\n",
    "\n",
    "\n",
    "```{admonition} Neuron's output (dot product)\n",
    "<p class=\"bottom-margin\">Using a dot product to express the weighted sum, the formula is:</p>\n",
    "\n",
    "$$\n",
    "\\small\n",
    "h\n",
    "=\n",
    "\\sigma\\left(\n",
    "\\begin{bmatrix}\n",
    "x_{1} & x_{2} & \\dots & x_{d}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "\\vdots \\\\\n",
    "w_{d}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "b\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "<p class=\"no-top-margin\">where:</p>\n",
    "\n",
    "- $d$: Dimensionality of the input vector.\n",
    "```\n",
    "\n",
    "```{important}\n",
    "Please note that the ouput of a neuron is one of the inputs to all the neurons in the next layer.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## PyTorch\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; To build our neural networks, we will use **PyTorch**, an open-source machine learning library widely used because of its flexibility, ease of use, and efficient computation. The following code sets up the environment for working with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set print options to 2 decimal places\n",
    "torch.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} PyTorch Tensors\n",
    "<p style=\"margin: 15px 1.4rem  15px 1.4rem ;\">PyTorch provides <strong>tensors</strong>, which are multi-dimensional arrays optimized for GPU processing. Depending on their dimensions, we will refer to them differently:</p>\n",
    "\n",
    "- A 1-dimensional tensor is called a **vector**. A vector with shape (3) would look like this:\n",
    "\n",
    "```python\n",
    "tensor([1, 2, 3])\n",
    "```\n",
    "\n",
    "- A 2-dimensional tensor is called a **matrix**. A matrix with shape (2, 3) would look like this:\n",
    "\n",
    "```python\n",
    "tensor([[1, 2, 3],\n",
    "        [4, 5, 6]])\n",
    "```\n",
    "\n",
    "- A 3 or more dimensional tensor is called an **n-dimensional tensor**. A 3-dimensional tensor with shape (2, 2, 3) would look like this:\n",
    "\n",
    "```python\n",
    "tensor([[[1, 2, 3],\n",
    "        [4, 5, 6]],\n",
    "        [[7, 8, 9],\n",
    "        [10, 11, 12]]])\n",
    "```\n",
    "````\n",
    "\n",
    "\n",
    "## How Neural Networks Learn\n",
    "\n",
    "Neural networks use supervised learning to fine-tune their parameters (weights and biases) by minimizing the loss function. This process involves repeating these four steps during training:\n",
    "\n",
    "1. **Forward Pass:** Process input data through the network to generate predictions.\n",
    "2. **Loss Function:** Measure the error by comparing predictions with actual target values.\n",
    "3. **Backward Pass:** Calculate gradients of the loss function with respect to the network's parameters using backpropagation.\n",
    "4. **Update Parameters:** Adjust weights and biases to reduce the loss, guided by the computed gradients.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "To better understand how neural networks learn, we will create a single-layer neural network that converts 3-digit binary numbers into their decimal equivalents.\n",
    "\n",
    "```{admonition} Conversion binary to decimal\n",
    "<p class=\"bottom-margin\">The conversion from a 3-digit binary number (<i>b2b1b0</i>) to its decimal equivalent (<i>D</i>) follows the formula:</p>\n",
    "\n",
    "$$\n",
    "\\small\n",
    "D = b_2 \\cdot 2^2 + b_1 \\cdot 2^1 + b_0 \\cdot 2^0\n",
    "$$\n",
    "\n",
    "<p class=\"no-top-margin\">where:</p>\n",
    "\n",
    "- $b_2, b_1, b_0$ are the binary digits 0 or 1. a\n",
    "\n",
    "The table below shows the equivalences:\n",
    "\n",
    "| Binary | Decimal |\n",
    "|--------|---------|\n",
    "| 000    | 0       |\n",
    "| 001    | 1       |\n",
    "| 010    | 2       |\n",
    "| 011    | 3       |\n",
    "| 100    | 4       |\n",
    "| 101    | 5       |\n",
    "| 110    | 6       |\n",
    "| 111    | 7       |\n",
    "\n",
    "```\n",
    "\n",
    "Although our neural network won't know this formula, it will learn the relationship between 3-digit binary and decimal numbers through training. We will use the following **input examples** and corresponding **labels** to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 0, 0], \n",
    "                  [0, 0, 0], \n",
    "                  [0, 0, 1], \n",
    "                  [0, 0, 1], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 1, 1], \n",
    "                  [0, 1, 1], \n",
    "                  [1, 0, 0], \n",
    "                  [1, 0, 0], \n",
    "                  [1, 0, 1], \n",
    "                  [1, 0, 1], \n",
    "                  [1, 1, 0], \n",
    "                  [1, 1, 0], \n",
    "                  [1, 1, 1], \n",
    "                  [1, 1, 1]]).float()\n",
    "\n",
    "labels = torch.tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{important}\n",
    "The input to a neuron must be floats (decimals)\n",
    "```\n",
    "\n",
    "### Network Structure\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; The neural network will have the following structure\n",
    "\n",
    "\n",
    "```{figure} ../images/binary-to-decimal.png\n",
    "---\n",
    "width: 340px\n",
    "name: binary-to-decimal\n",
    "---\n",
    "Structure of Neural Network\n",
    "```\n",
    "\n",
    "\n",
    "To begin, we will initialize the network parameters randomly. For simplicity, we will exclude bias terms and activation functions at this stage.\n",
    "\n",
    "```{admonition} Help\n",
    ":class: dropdown\n",
    "The `torch.randn` function generates a tensor filled with random numbers drawn from a standard normal distribution (mean = 0, standard deviation = 1). For more information, please refer to the [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.randn.html).\n",
    "```\n",
    "\n",
    "```{admonition} Help\n",
    ":class: dropdown\n",
    "The `requires_grad=True` argument indicates that the tensor should track gradients for operations. This will enable to run `loss.backward()` in the backward pass.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_nn():\n",
    "\n",
    "    # seed the random number generator for reproducibility\n",
    "    g = torch.Generator().manual_seed(1)\n",
    "\n",
    "    # intitialize randomly weight matrices and bias vectors\n",
    "    W1 = torch.randn((3, 10), generator=g)\n",
    "    b1 = torch.randn(10,      generator=g)\n",
    "    W2 = torch.randn((10, 8), generator=g)\n",
    "    b2 = torch.randn(8,       generator=g)\n",
    "\n",
    "    # list of parameters\n",
    "    parameters = [W1, b1, W2, b2]\n",
    "\n",
    "    # track gradients\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_nn()\n",
    "W1, b1, W2, b2 = parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; In the **forward pass**, the input data flows through the network, layer by layer, to produce the network's output.\n",
    "\n",
    "\n",
    "```{admonition} Neuron's output (dot product)\n",
    "<p class=\"bottom-margin\">In section <a href=\"#1.3\"><i>1.3. Neurons</i></a>, we saw that the output of a neuron is given by the formula:</p>\n",
    "\n",
    "$$\n",
    "\\small\n",
    "h\n",
    "=\n",
    "\\sigma\\left(\n",
    "\\begin{bmatrix}\n",
    "x_{1} & x_{2} & \\dots & x_{d}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "\\vdots \\\\\n",
    "w_{d}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "b\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "<p class=\"no-top-margin\">where:</p>\n",
    "\n",
    "- $d$: Dimensionality of the input vector.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "````{admonition} Layer's output (single example)\n",
    "<p class=\"bottom-margin\">We can adjust the previous formula and use a weight matrix to obtain the output of a layer, that is, the activation values of all the neurons in the layer:</p>\n",
    "\n",
    "$$\n",
    "\\small\n",
    "\\begin{bmatrix}\n",
    "h_{1} & h_{2} & \\dots & h_{m}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma\\left(\n",
    "\\begin{bmatrix}\n",
    "x_{1} & x_{2} & \\dots & x_{d}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\dots & w_{1m} \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{d1} & w_{d2} & \\dots & w_{dm} \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{1} & b_{2} & \\dots & b_{m}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "<p class=\"no-top-margin\">where:</p>\n",
    "\n",
    "- $d$: Dimensionality of the input vector.\n",
    "- $m$: Number of neurons in the layer.\n",
    "\n",
    "```{important}\n",
    "Each column of the weight matrix contains the weights of the connections between a single neuron in the layer and all the neurons in the previous layer.\n",
    "```\n",
    "\n",
    "````\n",
    "\n",
    "\n",
    "````{admonition} Layer's output (multiple example)\n",
    "<p class=\"bottom-margin\">Matrix multiplication enables us to efficiently calculate in parallel the output of a layer for several input examples:</p>\n",
    "\n",
    "$$\n",
    "\\tiny\n",
    "\\begin{bmatrix}\n",
    "h_{11} & h_{12} & \\dots & h_{1m} \\\\\n",
    "h_{21} & h_{22} & \\dots & h_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "h_{N1} & h_{N2} & \\dots & h_{Nm}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma\\left(\n",
    "\\begin{bmatrix}\n",
    "x_{11} & h_{12} & \\dots & h_{1d} \\\\\n",
    "x_{21} & h_{22} & \\dots & h_{2d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{N1} & h_{N2} & \\dots & h_{Nd}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\dots & w_{1m} \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{d1} & w_{d2} & \\dots & w_{dm} \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{1} & b_{2} & \\dots & b_{k}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "<p class=\"no-top-margin\">where:</p>\n",
    "\n",
    "- $d$: Dimensionality of the input vector.\n",
    "- $m$: Number of neurons in the layer.\n",
    "- $N$: Number of examples in a batch.\n",
    "\n",
    "```{important}\n",
    "Each row of the output matrix contains the activation values of all the neurons in the layer for a single input example.\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication\n",
    "h = x @ W1 + b1    # (16,10) = (16,3) x (3,10)\n",
    "o = h @ W2 + b2    # (16,8) = (16,10) x (10,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; A **loss function** is a mathematical representation that quantifies how well a machine learning model is performing. It measures the difference between the model's predicted outputs and the actual outputs from the dataset (the **targets**).\n",
    "\n",
    "There are various types of loss functions, each suitable for different tasks:\n",
    "- Regression Tasks: Mean Squared Error (MSE) and Mean Absolute Error (MAE).\n",
    "- Classification Tasks: Cross-Entropy Loss and Hinge Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.31\n"
     ]
    }
   ],
   "source": [
    "# calculate loss\n",
    "loss = F.cross_entropy(o, labels)\n",
    "print(f\"Loss: {loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backwards pass\n",
    "\n",
    "The network calculates gradients by propagating the error backward through the layers, determining how much each parameter contributed to the error. During **backpropagation**, the neural network computes the **gradient of the loss function** with respect to all its parameters by applying the chain rule of calculus.\n",
    "\n",
    "To compute all the gradients, we will use the PyTorch function `loss.backward()` which kept track of all the operations during the forwards pass. In  [](ch3.ipynb) we will perform a backward pass manually to better understand how backpropagation works and how the gradients are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset gradients to None\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "# calculate gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Parameters\n",
    "\n",
    "The calculated gradients are used to adjust the weights and biases through an optimization algorithm to minimize the loss function. Once the gradients have been computed, the neural network progresively **updates its parameters** trying minimize the loss function. The most common optimization techniques for updating the parameters are **gradient descent**, **stochastic gradient descent (SGD)**, and **Adam**.\n",
    "\n",
    "A **gradient** is a vector that represents the rate of change of a function with respect to its input variables. By default, gradients point in the **direction of steepest ascent**, that is, the direction in which the function increases the fastest. Since we want to minimize the loss function, we will move in the opposite direction of the gradients. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Gradient descent** is an optimization algorithm used to minimize a function, frequently used in neural networks training to minimize the loss function.\n",
    "\n",
    "Gradient descent updates each parameter by subtracting a fraction of the gradient from its current value, scaled by a factor known as the **learning rate**. The learning rate determines the size of the steps we take towards the minimum. A smaller learning rate results in more precise but slower convergence, while a larger learning rate can speed up convergence but may risk overshooting the minimum. The parameter update rule for gradient descent can be expressed mathematically as:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\nabla L(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ represents the parameters of the neural network\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\nabla L(\\theta)$ is the gradient of the loss function with respect to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1 # learning rate\n",
    "\n",
    "# update parameters\n",
    "for p in parameters:\n",
    "    p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "During **training**, a neural network iteratively performs a forward pass, calculates the loss, makes a backward pass, and updates its parameters. In this case, since the entire trainig dataset is processed in each training iteration, we can refer to each iteration as an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_nn()\n",
    "W1, b1, W2, b2 = parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0/100     Loss: 11.3085\n",
      "Step: 10/100     Loss: 2.3920\n",
      "Step: 20/100     Loss: 1.2069\n",
      "Step: 30/100     Loss: 0.7290\n",
      "Step: 40/100     Loss: 0.5013\n",
      "Step: 50/100     Loss: 0.3736\n",
      "Step: 60/100     Loss: 0.2934\n",
      "Step: 70/100     Loss: 0.2389\n",
      "Step: 80/100     Loss: 0.1999\n",
      "Step: 90/100     Loss: 0.1708\n",
      "Step: 99/100     Loss: 0.1504\n"
     ]
    }
   ],
   "source": [
    "epochs = 100      # train iterations\n",
    "lr = 0.1            # learning rate\n",
    "\n",
    "# list to keep track of the loss in each step\n",
    "loss_graph = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # forward pass\n",
    "    h = x @ W1 + b1    # (16,10) = (16,3) x (3,10)\n",
    "    o = h @ W2 + b2    # (16,8) = (16,10) x (10,8)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = F.cross_entropy(o, labels)\n",
    "    loss_graph.append(loss.item())\n",
    "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Step: {epoch:2d}/{epochs}     Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.tensor([1,0,1]).float()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = test @ W1 + b1    # (16,10) = (1,3) x (3,10)\n",
    "o = h1 @ W2 + b2    # (16,8) = (16,10) x (10,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
