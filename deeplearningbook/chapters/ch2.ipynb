{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "\n",
    "## How Neural Networks Learn\n",
    "\n",
    "Neural networks use supervised learning to fine-tune their parameters (weights and biases) by minimizing the loss function. This process involves repeating these four steps during training:\n",
    "\n",
    "1. **Forward Pass:** Process input data through the network to generate predictions.\n",
    "2. **Loss Function:** Measure the error by comparing predictions with actual target values.\n",
    "3. **Backward Pass:** Calculate gradients of the loss function with respect to the network's parameters using backpropagation.\n",
    "4. **Update Parameters:** Adjust weights and biases to reduce the loss, guided by the computed gradients.\n",
    "\n",
    "\n",
    "## Digits Recognizer\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; In this chapter we will build a discriminative model to recognize handwritten digits. To train our neural network, we will use the [MNIST dataset](https://huggingface.co/datasets/ylecun/mnist), which has 60,000 training images and 10,000 test images. All the images are grayscale and 28 x 28 pixels.\n",
    "\n",
    "```{note}\n",
    "This chapter will focus on key concepts and challeneges of deep neural networks. Chapter 4 will delve more into working with images and image classification techniques.\n",
    "```\n",
    "\n",
    "```{figure} ../images/digit-recognizer.png\n",
    "---\n",
    "width: 410px\n",
    "name: digit-recognizer\n",
    "---\n",
    "Digit Recognizer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# convert PIL image to normalized PyToch tensor\n",
    "def image_to_tensor(image):\n",
    "    return torch.tensor(np.array(image)) / 255.0\n",
    "\n",
    "\n",
    "def preprocess_data(split):\n",
    "    \n",
    "    x = []  # list to store image tensors\n",
    "    y = []  # list to store labels\n",
    "\n",
    "    for example in split:\n",
    "        x.append(image_to_tensor(example['image']))\n",
    "        y.append(example['label'])\n",
    "    \n",
    "    return torch.stack(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: torch.Size([60000, 28, 28])\n",
      "Labels: torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# set print options for PyTorch tensors\n",
    "torch.set_printoptions(linewidth=140, sci_mode=False, precision=4)\n",
    "\n",
    "# load the MNIST dataset\n",
    "ds = load_dataset(\"ylecun/mnist\")\n",
    "\n",
    "# preprocess the data (we will see this function in Chapter 4)\n",
    "train_x, train_y = preprocess_data(ds['train'])\n",
    "\n",
    "print(f\"Images: {train_x.shape}\")\n",
    "print(f\"Labels: {train_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform input tensor from (60000, 28, 28) to (60000, 784)\n",
    "X = train_x.view(-1, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; A **multilayer perceptron (MLP)** is a type of neural network composed of multiple layers of fully connected neurons with nonlinear activation functions. Our MLP will have 784 input neurons (one for each pixel in the image) and 10 output neurons (one for each possible class: 0 to 9).\n",
    "\n",
    "```{figure} ../images/MLP.png\n",
    "---\n",
    "width: 450px\n",
    "name: MLP\n",
    "---\n",
    "MLP\n",
    "```\n",
    "\n",
    "```{admonition} Help\n",
    ":class: dropdown\n",
    "The `torch.randn` function generates a tensor filled with random numbers drawn from a standard normal distribution (mean = 0, standard deviation = 1). For more information, please refer to the [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.randn.html).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_nn(n_hidden = 100):\n",
    "\n",
    "    g = torch.Generator().manual_seed(1)\n",
    "    \n",
    "    W1 = torch.randn((784, n_hidden),      generator=g)\n",
    "    b1 = torch.zeros(n_hidden)\n",
    "    W2 = torch.randn((n_hidden, n_hidden), generator=g)\n",
    "    b2 = torch.zeros(n_hidden)\n",
    "    W3 = torch.randn((n_hidden, 10),       generator=g)\n",
    "    b3 = torch.zeros(10)\n",
    "\n",
    "    parameters = [W1, b1, W2, b2, W3, b3]\n",
    "\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_nn()\n",
    "W1, b1, W2, b2, W3, b3 = parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; In the **forward pass**, the input data flows through the neural network, layer by layer, to produce the network's output, known as logits.\n",
    "\n",
    "```{admonition} Neuron's output (dot product)\n",
    "<p class=\"bottom-margin\">In section <a href=\"#1.3\"><i>1.3. Neurons</i></a>, we saw that the output of a neuron is given by the formula:</p>\n",
    "\n",
    "$$\n",
    "\\small\n",
    "h\n",
    "=\n",
    "\\sigma\\left(\n",
    "\\begin{bmatrix}\n",
    "x_{1} & x_{2} & \\dots & x_{d}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "\\vdots \\\\\n",
    "w_{d}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "b\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "<p class=\"no-top-margin\">where:</p>\n",
    "\n",
    "- $d$: Dimensionality of the input vector.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "````{admonition} Layer's output (single examples)\n",
    "<p class=\"bottom-margin\">We can adjust the previous formula and use a weight matrix to obtain the output of a layer, that is, the activation values of all the neurons in the layer:</p>\n",
    "\n",
    "$$\n",
    "\\small\n",
    "\\begin{bmatrix}\n",
    "h_{1} & h_{2} & \\dots & h_{m}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma\\left(\n",
    "\\begin{bmatrix}\n",
    "x_{1} & x_{2} & \\dots & x_{d}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\dots & w_{1m} \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{d1} & w_{d2} & \\dots & w_{dm} \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{1} & b_{2} & \\dots & b_{m}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "<p class=\"no-top-margin\">where:</p>\n",
    "\n",
    "- $d$: Dimensionality of the input vector.\n",
    "- $m$: Number of neurons in the layer.\n",
    "\n",
    "```{important}\n",
    "Each column of the weight matrix contains the weights of the connections between a single neuron in the layer and all the neurons in the previous layer.\n",
    "```\n",
    "\n",
    "````\n",
    "\n",
    "\n",
    "````{admonition} Layer's output (multiple example)\n",
    "<p class=\"bottom-margin\">Matrix multiplication enables us to efficiently calculate in parallel the output of a layer for several input examples:</p>\n",
    "\n",
    "$$\n",
    "\\tiny\n",
    "\\begin{bmatrix}\n",
    "h_{11} & h_{12} & \\dots & h_{1m} \\\\\n",
    "h_{21} & h_{22} & \\dots & h_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "h_{N1} & h_{N2} & \\dots & h_{Nm}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma\\left(\n",
    "\\begin{bmatrix}\n",
    "x_{11} & h_{12} & \\dots & h_{1d} \\\\\n",
    "x_{21} & h_{22} & \\dots & h_{2d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{N1} & h_{N2} & \\dots & h_{Nd}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\dots & w_{1m} \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{d1} & w_{d2} & \\dots & w_{dm} \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{1} & b_{2} & \\dots & b_{k}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "<p class=\"no-top-margin\">where:</p>\n",
    "\n",
    "- $d$: Dimensionality of the input vector.\n",
    "- $m$: Number of neurons in the layer.\n",
    "- $N$: Number of examples in a batch.\n",
    "\n",
    "```{important}\n",
    "Each row of the output matrix contains the activation values of all the neurons in the layer for a single input example.\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "### Tanh\n",
    "\n",
    "\n",
    "### Softmax\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; **Softmax** is an activation function often used in the output layer of neural networks. It transforms raw neural network outputs, known as logits, into probability distributions where each probability represents the model's confidence that a given example belongs to a specific class.\n",
    "\n",
    "```{figure} ../images/softmax.png\n",
    "---\n",
    "width: 500px\n",
    "name: softmax\n",
    "---\n",
    "Softmax\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer 1\n",
    "h1_pre = X @ W1 + b1\n",
    "h1 = torch.tanh(h1_pre)\n",
    "\n",
    "# hidden layer 2\n",
    "h2_pre = h1 @ W2 + b2\n",
    "h2 = torch.tanh(h2_pre)\n",
    "\n",
    "# output layer\n",
    "logits = h2 @ W3 + b3\n",
    "probs = logits.exp() / logits.exp().sum(1, keepdims=True)  # softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Loss\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; In Chapter 2, we defined the forward pass as the step where input data flows through the neural network, layer by layer, to produce the network's output. Now, we are going to see the forward pass as the step where the model, using its current parameters, generates predictions. \n",
    "\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; **Cross-Entropy Loss** is a widely used loss function for classification tasks that evaluates how well these predictions align with the labels. To calculate the loss, we often use PyTorch's function `F.cross_entropy`, which efficiently applies Softmax and calculates the average NLL.\n",
    "\n",
    "\n",
    "### Average NLL\n",
    "\n",
    "```{admonition} Likelihood\n",
    "\n",
    "The **likelihood** is the product of all the probabilities assigned by the model to the correct classes:\n",
    "\n",
    "$$\n",
    "\\text{likelihood} = \\prod_{i=1}^{N} p_i\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $p_i$ is the probability assigned by the model to the correct class for the $i$-th example.  \n",
    "- $N$ is the total number of examples in the dataset.\n",
    "```\n",
    "\n",
    "```{admonition} Log Likelihood\n",
    "\n",
    "Since each $p_i$ is a value between 0 and 1, their product can become very small. To avoid numerical issues, we use the **log likelihood**:\n",
    "\n",
    "$$\n",
    "\\text{log likelihood} = \\log \\left(\\prod_{i=1}^{N} p_i \\right)\n",
    "$$\n",
    "\n",
    "Using the product property of logarithms, this can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\text{log likelihood} = \\sum_{i=1}^{N} \\log(p_i)\n",
    "$$\n",
    "```\n",
    "\n",
    "\n",
    "````{admonition} Negative Log Likelihood\n",
    "\n",
    "Looking at the graph of the logarithmic function, please note that:\n",
    "\n",
    "- If we pass in a probability of $1$, the log probability is $0$.\n",
    "- If we pass in a lower probability $\\left(0 < p < 1 \\right)$, the log probability becomes more negative.  \n",
    "- If we pass in a probability of 0, the log probability is $-\\infty$.\n",
    "\n",
    "```{figure} ../images/log-function.png\n",
    "---\n",
    "width: 250px\n",
    "name: log-function\n",
    "---\n",
    "Logarithmic Function\n",
    "```\n",
    "\n",
    "Thus, when all the individual probabilities are 1 (the best-case scenario), the log likelihood is 0, and when the probabilities decrease, the log likelihood becomes more negative. To make it eassier to interpret, we use the **negative log likelihood (NLL)**, a positive metric where values closer to 0 indicate better predictions:\n",
    "\n",
    "$$\n",
    "\\text{NLL} = - \\sum_{i=1}^{N} \\log(p_i)\n",
    "$$\n",
    "\n",
    "````\n",
    "\n",
    "```{admonition} Average Negative Log Likelihood\n",
    "\n",
    "To normalize the NLL, it is often divided by the total number of examples in the dataset, resulting in the **average negative log likelihood**:\n",
    "\n",
    "$$\n",
    "\\text{Average NLL} = - \\frac{1}{N} \\sum_{i=1}^{N} \\log(p_i)\n",
    "$$\n",
    "\n",
    "```\n",
    "\n",
    "Thus, the aveage NLL is a value that is closer to 0 when the probabilies assigned by the model to the correct classes are closer to 1 for all the examples.\n",
    "\n",
    "```{admonition} Help\n",
    ":class: dropdown\n",
    "`torch.view()`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 15.9699\n"
     ]
    }
   ],
   "source": [
    "# probability assigned by the model to the correct class for the ith example\n",
    "pi = probs[(torch.arange(logits.shape[0]), train_y)]\n",
    "\n",
    "# average negative log liklihood\n",
    "loss = -pi.log().mean()\n",
    "\n",
    "print(f\"Train Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.9699, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden layer 1\n",
    "h1_pre = X @ W1 + b1\n",
    "h1 = torch.tanh(h1_pre)\n",
    "\n",
    "# hidden layer 2\n",
    "h2_pre = h1 @ W2 + b2\n",
    "h2 = torch.tanh(h2_pre)\n",
    "\n",
    "# output layer\n",
    "logits = h2 @ W3 + b3\n",
    "\n",
    "# softmax\n",
    "counts = logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1     # same as 1.0 / counts_sum\n",
    "probs = counts * counts_sum_inv\n",
    "\n",
    "# average negative log liklihood\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(N), train_y].mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "  p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "          h1, h1_pre, h2, h2_pre, logits]:\n",
    "  t.retain_grad()\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss is the negative mean of all the log probs:\n",
    "\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "<br>\n",
    "\n",
    "- Let us simplify the problem to *loss = -(a + b + c) / 3*. \n",
    "\n",
    "- Algebraically, *loss = -1/3a -1/3b -1/3c*. \n",
    "\n",
    "- Then, the derivative of the loss with respect to a (or b or c) is *dloss/da = -1/3*. \n",
    "\n",
    "- More generally *dloss/da = -1/n*.\n",
    "\n",
    "`dlogprobs` will hold the derivative of the loss with respect to all the elements of `logprobs`.\n",
    "\n",
    "\n",
    "\n",
    "Thus, the derivative of the loss with respect to the **log probabilities of the correct next characters** is -1/n.\n",
    "\n",
    "The **log probabilties of the incorrect next characters** do not participate in the calculation of the loss. Thus, the derivative of the loss with respect to them is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs) # matrix of zeros with the shape of logprobs\n",
    "dlogprobs[range(N), train_y] = -1.0 / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "logprobs = log(probs) \\quad \\Rightarrow \\quad dprobs = \\frac{1}{probs} \\cdot dlogprobs\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprobs = (1.0 / probs) * dlogprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "probs = counts \\cdot counts\\_sum\\_inv \\quad \\Rightarrow \\quad\n",
    "\\begin{matrix}\n",
    "dcounts = counts\\_sum\\_inv \\cdot dprobs \\\\\n",
    "dcounts\\_sum\\_inv = counts \\cdot dprobs\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "counts\\_sum\\_inv = \\frac{1}{counts\\_sum} \\quad \\Rightarrow \\quad dcounts\\_sum = -\\frac{1}{\\sqrt{counts\\_sum}} \\cdot dcounts\\_sum\\_inv\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "counts\\_sum = \\text{sum of rows of counts} \\quad \\Rightarrow \\quad dcounts = dcounts\\_sum\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts += torch.ones_like(counts) * dcounts_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "counts = e^{logits} \\quad \\Rightarrow \\quad dlogits = counts \\cdot dcounts\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits = counts * dcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{logits} = h2 \\times W3 + b3 \\quad \\Rightarrow \\quad\n",
    "\\begin{matrix}\n",
    "dh2 = dlogits \\times (W3)^T \\\\\n",
    "dW3 = (h2)^T \\times dlogits \\\\\n",
    "db3 = \\text{sum of columns of dlogits}\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh2 = dlogits @ W3.T\n",
    "dW3 = h2.T @ dlogits\n",
    "db3 = dlogits.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h2 = tanh(h2\\_pre) \\quad \\Rightarrow \\quad dh2\\_pre = (1 - (h2)^2) \\cdot dh2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh2_pre = (1.0 - h2**2) * dh2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h2\\_pre = h1 \\times W2 + b2 \\quad \\Rightarrow \\quad\n",
    "\\begin{matrix}\n",
    "dh1 = dh2\\_pre \\times (W2)^T \\\\\n",
    "dW2 = (h1)^T \\times dh2\\_pre \\\\\n",
    "db2 = \\text{sum of columns of dh2\\_pre}\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh1 = dh2_pre @ W2.T\n",
    "dW2 = h1.T @ dh2_pre\n",
    "db2 = dh2_pre.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h1 = tanh(h1\\_pre) \\quad \\Rightarrow \\quad dh1\\_pre = (1 - (h1)^2) \\cdot dh1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh1_pre = (1.0 - h1**2) * dh1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h1\\_pre = X \\times W1 + b1 \\quad \\Rightarrow \\quad\n",
    "\\begin{matrix}\n",
    "dh1 = dh1\\_pre \\times (W1)^T \\\\\n",
    "dW1 = (X)^T \\times dh1\\_pre \\\\\n",
    "db1 = \\text{sum of columns of dh1\\_pre}\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX = dh1_pre @ W1.T\n",
    "dW1 = X.T @ dh1_pre\n",
    "db1 = dh1_pre.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": [
     "hide-input",
     "scroll-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0/100     Loss: 15.9699\n",
      "Epoch: 10/100     Loss: 10.9262\n",
      "Epoch: 20/100     Loss: 8.3451\n",
      "Epoch: 30/100     Loss: 6.7141\n",
      "Epoch: 40/100     Loss: 5.6266\n",
      "Epoch: 50/100     Loss: 4.8790\n",
      "Epoch: 60/100     Loss: 4.3457\n",
      "Epoch: 70/100     Loss: 3.9500\n",
      "Epoch: 80/100     Loss: 3.6406\n",
      "Epoch: 90/100     Loss: 3.3900\n",
      "Epoch: 99/100     Loss: 3.2024\n"
     ]
    }
   ],
   "source": [
    "epochs = 100       # train iterations\n",
    "lr = 0.1           # learning rate\n",
    "\n",
    "# intialize neural network\n",
    "parameters = initialize_nn()\n",
    "W1, b1, W2, b2, W3, b3 = parameters\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # -------------------- forward pass --------------------\n",
    "\n",
    "    # hidden layer 1\n",
    "    h1_pre = X @ W1 + b1\n",
    "    h1 = torch.tanh(h1_pre)\n",
    "\n",
    "    # hidden layer 2\n",
    "    h2_pre = h1 @ W2 + b2\n",
    "    h2 = torch.tanh(h2_pre)\n",
    "\n",
    "    # output layer\n",
    "    logits = h2 @ W3 + b3\n",
    "\n",
    "    # softmax\n",
    "    counts = logits.exp()\n",
    "    counts_sum = counts.sum(1, keepdims=True)\n",
    "    counts_sum_inv = counts_sum**-1\n",
    "    probs = counts * counts_sum_inv\n",
    "\n",
    "\n",
    "    # -------------------- calculate loss --------------------\n",
    "\n",
    "    # average negative log liklihood\n",
    "    logprobs = probs.log()\n",
    "    loss = -logprobs[range(N), train_y].mean()\n",
    "\n",
    "    # print loss every 10 epochs\n",
    "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch: {epoch:2d}/{epochs}     Loss: {loss.item():.4f}\")\n",
    "    \n",
    "\n",
    "    # -------------------- backward pass --------------------\n",
    "\n",
    "    dlogprobs = torch.zeros_like(logprobs)\n",
    "    dlogprobs[range(N), train_y] = -1.0 / N\n",
    "\n",
    "    dprobs = (1.0 / probs) * dlogprobs\n",
    "    dcounts = counts_sum_inv * dprobs\n",
    "    dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "    dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "    dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "    dlogits = counts * dcounts\n",
    "    dh2 = dlogits @ W3.T\n",
    "    dW3 = h2.T @ dlogits\n",
    "    db3 = dlogits.sum(0)\n",
    "    dh2_pre = (1.0 - h2**2) * dh2\n",
    "    dh1 = dh2_pre @ W2.T\n",
    "    dW2 = h1.T @ dh2_pre\n",
    "    db2 = dh2_pre.sum(0)\n",
    "    dh1_pre = (1.0 - h1**2) * dh1\n",
    "    dX = dh1_pre @ W1.T\n",
    "    dW1 = X.T @ dh1_pre\n",
    "    db1 = dh1_pre.sum(0)\n",
    "\n",
    "    grads = [dW1, db1, dW2, db2, dW3, db3]\n",
    "\n",
    "\n",
    "    # -------------------- update parameters --------------------\n",
    "\n",
    "    for p, grad in zip(parameters, grads):\n",
    "        p.data += -lr * grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
