{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; **Neural networks (NNs)** are computational models inspired by the structure of the human brain, designed to recognize patterns and make predictions. They consist of layers of interconnected nodes (often called neurons) that process information through mathematical operations.\n",
    "\n",
    "A basic neural network has the following structure:\n",
    "\n",
    "1. **Input Layer**: It receives raw data, like pixels of an image, or tokens of a text. Each node in this layer represents an input dimension.\n",
    "2. **Hidden Layers**: They are between the input layer and output layer, and process the information. Each hidden layer transforms data from the previous layer, allowing the network to progressively learn and recognize patterns.\n",
    "3. **Output Layer**: It provides the networkâ€™s output.\n",
    "\n",
    "```{figure} ../images/neural-network.png\n",
    "---\n",
    "width: 300px\n",
    "name: neural-network\n",
    "---\n",
    "Basic Structure of a Neural Network\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "(2.2)=\n",
    "## Neurons\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; A **neuron** is a unit that takes in multiple inputs and processes them to produce a single output. As shown above in [*Fig. 1.2*](neural-network), each neuron in the hidden and output layers connects to all the neurons in the previous layer. These connections have associated values, known as **weights**, which are adjusted by the model. The weights represent the strength of connection between the neurons. A greater weight indicates a stronger connection.\n",
    "\n",
    "\n",
    "```{figure} ../images/neuron.png\n",
    "---\n",
    "width: 220px\n",
    "name: neuron\n",
    "---\n",
    "Structure of a Neuron\n",
    "```\n",
    "\n",
    "\n",
    "```{admonition} Neuron's output\n",
    "\n",
    "To calculate the output of a neuron, all the inputs to the neuron ($x_{1}, x_{2}, \\dots, x_{d}$) are multiplied by their corresponding connection weights ($w_{1}, w_{2}, \\dots, w_{d}$), and the products are summed. \n",
    "\n",
    "A numerical value, called the bias ($b$), is then added to the weighted sum. Finally, the result is passed through an **activation function** ($\\sigma$) that returns the output of the neuron, the neuron's **activation value** ($h$).\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\small\n",
    "h = \\sigma\\left(x_1w_1 + x_2w_2 + \\dots + x_dw_d + b\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $d$ is the dimensionality of the input vector.\n",
    "```\n",
    "\n",
    "\n",
    "```{important}\n",
    "Please note that the ouput of a neuron is one of the inputs to all the neurons in the next layer.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How NNs Learn\n",
    "\n",
    "Neural networks use supervised learning to adjust their parameters (weights and biases) to minimize the loss function. This process involves repeating these four steps during training:\n",
    "\n",
    "1. **Forward Pass:** Process input data though network and generate predictions.\n",
    "2. **Calculate Loss:** Calculate the error of the predictions.\n",
    "3. **Backward Pass:** Compute gradient of the loss function with respect to the network's parameters.\n",
    "4. **Update Parameters:** Adjust network's parameters to minimize the loss.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Digits Recognizer\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; In this and the following chapters, we will build a discriminative model to recognize handwritten digits. To train our neural network, we will use the [MNIST dataset](https://huggingface.co/datasets/ylecun/mnist), which contains 60,000 training images and 10,000 test images. All the images are labeled, grayscale, and 28 x 28 pixels.\n",
    "\n",
    "\n",
    "```{figure} ../images/digit-recognizer.png\n",
    "---\n",
    "width: 400px\n",
    "name: digit-recognizer\n",
    "---\n",
    "Digit Recognizer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# convert PIL image to normalized PyToch tensor\n",
    "def image_to_tensor(image):\n",
    "    return torch.tensor(np.array(image)) / 255.0\n",
    "\n",
    "\n",
    "def preprocess_data(split):\n",
    "    \n",
    "    x = []  # list to store image tensors\n",
    "    y = []  # list to store labels\n",
    "\n",
    "    for example in split:\n",
    "        x.append(image_to_tensor(example['image']))\n",
    "        y.append(example['label'])\n",
    "    \n",
    "    return torch.stack(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: torch.Size([60000, 28, 28])\n",
      "Labels: torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "# set print options for PyTorch tensors\n",
    "torch.set_printoptions(linewidth=140, sci_mode=False, precision=4)\n",
    "\n",
    "# load the MNIST dataset\n",
    "ds = load_dataset(\"ylecun/mnist\")\n",
    "\n",
    "# preprocess the data (we will see this function in Chapter 4)\n",
    "train_x, train_y = preprocess_data(ds['train'])\n",
    "\n",
    "print(f\"Images: {train_x.shape}\")\n",
    "print(f\"Labels: {train_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is currently stored as a matrix of shape (28, 28). To be able to process these images, we need to flatten them into vectors of shape (784).\n",
    "\n",
    "```{admonition} Help\n",
    ":class: dropdown\n",
    "`tensor.view()` efficiently reshapes the shape of a tensor.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# flatten images\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_x\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m784\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# number of images \u001b[39;00m\n\u001b[0;32m      5\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60000\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "# flatten images\n",
    "X = train_x.view(-1, 784)\n",
    "\n",
    "print(f\"Images: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; A **multilayer perceptron (MLP)** is a type of neural network composed of multiple layers of fully connected neurons with nonlinear activation functions. Our MLP will have 784 input neurons (one for each pixel in the image) and 10 output neurons (one for each possible class: 0 to 9).\n",
    "\n",
    "\n",
    "```{figure} ../images/MLP.png\n",
    "---\n",
    "width: 400px\n",
    "name: MLP\n",
    "---\n",
    "MLP\n",
    "```\n",
    "\n",
    "\n",
    "```{admonition} Help\n",
    ":class: dropdown\n",
    "`torch.randn()` generates a tensor filled with random numbers drawn from a normal distribution (mean = 0, standard deviation = 1).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_nn(n_hidden = 100):\n",
    "\n",
    "    # seed for reproducibility\n",
    "    g = torch.Generator().manual_seed(1)\n",
    "    \n",
    "    W1 = torch.randn((784, n_hidden),      generator=g)\n",
    "    b1 = torch.zeros(n_hidden)\n",
    "    W2 = torch.randn((n_hidden, n_hidden), generator=g)\n",
    "    b2 = torch.zeros(n_hidden)\n",
    "    W3 = torch.randn((n_hidden, 10),       generator=g)\n",
    "    b3 = torch.zeros(10)\n",
    "\n",
    "    parameters = [W1, b1, W2, b2, W3, b3]\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_nn()\n",
    "W1, b1, W2, b2, W3, b3 = parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "In the **forward pass**, the input data flows through the neural network, layer by layer, to produce the network's output.\n",
    "\n",
    "\n",
    "```{admonition} Neuron's output\n",
    "\n",
    "In section [2.2](2.2), we saw that the output of a neuron is given by the formula:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\small\n",
    "h = \\sigma\\left(x_1w_1 + x_2w_2 + \\dots + x_dw_d + b\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $d$ is the dimensionality of the input vector.\n",
    "```\n",
    "\n",
    "\n",
    "```{admonition} Neuron's output (dot product)\n",
    "\n",
    "Using a dot product we can express the weighted sum as:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\small\n",
    "h = \\sigma\\left(\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & \\dots & x_d\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_d\n",
    "\\end{bmatrix}\n",
    "+ b\\right)\n",
    "$$\n",
    "```\n",
    "\n",
    "\n",
    "````{admonition} Layer's output (single examples)\n",
    "\n",
    "Using a weight matrix we can calculate the activation values of all the neurons in the layer:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\small\n",
    "\\begin{bmatrix}\n",
    "h_1 & h_2 & \\dots & h_m\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma\\left(\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & \\dots & x_d\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\dots & w_{1m} \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{d1} & w_{d2} & \\dots & w_{dm} \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 & \\dots & b_m\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $m$ is the number of neurons in the layer.\n",
    "\n",
    "\n",
    "```{important}\n",
    "Each column of the weight matrix contains the weights of the connections between a single neuron in the current layer and all the neurons in the previous layer.\n",
    "```\n",
    "\n",
    "````\n",
    "\n",
    "\n",
    "````{admonition} Layer's output (multiple examples)\n",
    "\n",
    "Uisng matrix multiplication we can efficiently calculate in parallel the output of a layer for several input examples:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\small\n",
    "\\begin{bmatrix}\n",
    "h_{11} & h_{12} & \\dots & h_{1m} \\\\\n",
    "h_{21} & h_{22} & \\dots & h_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "h_{N1} & h_{N2} & \\dots & h_{Nm}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma\\left(\n",
    "\\begin{bmatrix}\n",
    "x_{11} & h_{12} & \\dots & h_{1d} \\\\\n",
    "x_{21} & h_{22} & \\dots & h_{2d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{N1} & h_{N2} & \\dots & h_{Nd}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\dots & w_{1m} \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{d1} & w_{d2} & \\dots & w_{dm} \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{1} & b_{2} & \\dots & b_{m}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $N$ is the number of input examples.\n",
    "\n",
    "\n",
    "```{important}\n",
    "Each row of the output matrix contains the activation values of all the neurons in the layer for a single input example.\n",
    "```\n",
    "\n",
    "````\n",
    "\n",
    "\n",
    "## Tanh\n",
    "\n",
    "\n",
    "## Softmax\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; **Softmax** is an activation function often used in the output layer of neural networks. It transforms raw neural network outputs, known as logits, into probability distributions where each probability represents the model's confidence that a given example belongs to a specific class.\n",
    "\n",
    "```{figure} ../images/softmax.png\n",
    "---\n",
    "width: 500px\n",
    "name: softmax\n",
    "---\n",
    "Softmax\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.6741,     0.3209,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0050,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.8520,     0.0000,     0.1480],\n",
      "        [    0.0000,     1.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0005,     0.0000,     0.9965,     0.0000,     0.0000,     0.0001,     0.0028,     0.0000],\n",
      "        [    0.9911,     0.0000,     0.0033,     0.0000,     0.0000,     0.0000,     0.0056,     0.0000,     0.0000,     0.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "h1 = torch.tanh(X @ W1 + b1)   # (60000, 100) = (60000, 784) x (784, 100) + (100)\n",
    "h2 = torch.tanh(h1 @ W2 + b2)  # (60000, 100) = (60000, 100) x (100, 100) + (100)\n",
    "logits = h2 @ W3 + b3          # (60000,  10) = (60000, 100) x (100,  10) + (10)\n",
    "\n",
    "# softmax\n",
    "probs = logits.exp() / logits.exp().sum(1, keepdims=True)  # (60000,  10)\n",
    "\n",
    "# print  first 5 probabilities\n",
    "print(probs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Loss\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; The forward pass can also be seen as the step where the model, using its current parameters, generates predictions. The **Cross-Entropy Loss** is a widely used loss function for classification tasks that evaluates how well these predictions align with the labels. It combines Softmax with the average negative log-likelihood.\n",
    "\n",
    "```{admonition} Likelihood\n",
    "\n",
    "The **likelihood** represents the joint probability of the model assigning correct labels to all examples:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{likelihood} = \\prod_{i=1}^{N} p_i\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $p_i$ is the probability assigned by the model to the correct class for the $i$-th example.  \n",
    "- $N$ is the total number of examples in the dataset.\n",
    "```\n",
    "\n",
    "```{admonition} Log Likelihood\n",
    "\n",
    "Since each $p_i$ is a value between 0 and 1, their product can become very small. To avoid numerical instability, we take the logarithm of the likelihood:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{log likelihood} = \\log \\left(\\prod_{i=1}^{N} p_i \\right) = \\sum_{i=1}^{N} \\log(p_i)\n",
    "$$\n",
    "```\n",
    "\n",
    "\n",
    "````{admonition} Negative Log Likelihood\n",
    "\n",
    "Looking at the graph of the logarithmic function, please note that:\n",
    "\n",
    "- If we pass in a probability of $1$, the log probability is $0$.\n",
    "- If we pass in a lower probability $\\left(0 < p < 1 \\right)$, the log probability becomes more negative.  \n",
    "- If we pass in a probability of 0, the log probability is $-\\infty$.\n",
    "\n",
    "```{figure} ../images/log-function.png\n",
    "---\n",
    "width: 200px\n",
    "name: log-function\n",
    "---\n",
    "Logarithmic Function\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, when all the individual probabilities are 1 (the best-case scenario), the log likelihood is 0, and when the probabilities decrease, the log likelihood becomes more negative. To make it eassier to interpret, we use the **negative log likelihood (NLL)**, a positive metric where values closer to 0 indicate better predictions:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{NLL} = - \\sum_{i=1}^{N} \\log(p_i)\n",
    "$$\n",
    "\n",
    "````\n",
    "\n",
    "```{admonition} Average Negative Log Likelihood\n",
    "\n",
    "To normalize the NLL, it is often divided by the total number of examples in the dataset:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{Average NLL} = - \\frac{1}{N} \\sum_{i=1}^{N} \\log(p_i)\n",
    "$$\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 15.9699\n"
     ]
    }
   ],
   "source": [
    "# number of train images \n",
    "N = 60000\n",
    "\n",
    "# average NLL\n",
    "loss = -probs[range(N), train_y].log().mean()\n",
    "\n",
    "print(f\"Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; A lower loss indicates that the model is making more accurate predictions by assigning higher probabilities to the correct classes. Thus, to improve the model's performance, we aim to minimize the loss by adjusting its parameters. \n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; During the **backward pass**, or backpropagation, we calculate the gradient of the loss function with respect to these parameters. This gradient is determined by computing the derivative of the loss with respect to the model's parameters using the chain rule. To simplify this process, we will break the forward pass into smaller components, making it easier to differentiate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer 1\n",
    "h1_pre = X @ W1 + b1\n",
    "h1 = torch.tanh(h1_pre)\n",
    "\n",
    "# hidden layer 2\n",
    "h2_pre = h1 @ W2 + b2\n",
    "h2 = torch.tanh(h2_pre)\n",
    "\n",
    "# output layer\n",
    "logits = h2 @ W3 + b3\n",
    "\n",
    "# softmax\n",
    "counts = logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "\n",
    "# average NLL\n",
    "log_probs = probs.log()\n",
    "loss = -log_probs[range(N), train_y].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first calculate the derivative of the loss with respect to the log probabilities.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "loss = - mean(log\\_probs) \\quad \\Rightarrow \\quad\n",
    "\\begin{matrix}\n",
    "dlog\\_probs = 0 \\text{ if not p_i}\\\\\n",
    "dlog\\_probs = -\\frac{1}{N} \\text{if p_i}\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlog_probs = torch.zeros_like(log_probs)\n",
    "dlog_probs[range(N), train_y] = -1.0 / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the derivative of the loss with respect to the probabilities using the chain rule.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "log\\_probs = log(probs) \\quad \\Rightarrow \\quad dprobs = \\frac{1}{probs} \\cdot dlog\\_probs\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprobs = (1.0 / probs) * dlog_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue backpropagating by calculating the derivative of the loss with respect to the intermediate values, and then, with respect to the model's parameters.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "probs = counts \\cdot counts\\_sum\\_inv \\quad \\Rightarrow \\quad\n",
    "\\begin{matrix}\n",
    "dcounts = counts\\_sum\\_inv \\cdot dprobs \\\\\n",
    "dcounts\\_sum\\_inv = counts \\cdot dprobs\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "counts\\_sum\\_inv = \\frac{1}{counts\\_sum} \\quad \\Rightarrow \\quad dcounts\\_sum = -\\frac{1}{\\sqrt{counts\\_sum}} \\cdot dcounts\\_sum\\_inv\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "counts\\_sum = \\text{sum of rows of counts} \\quad \\Rightarrow \\quad dcounts = dcounts\\_sum\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts += torch.ones_like(counts) * dcounts_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "counts = e^{logits} \\quad \\Rightarrow \\quad dlogits = counts \\cdot dcounts\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits = counts * dcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{logits} = h2 \\times W3 + b3 \\quad \\Rightarrow \\quad\n",
    "\\begin{matrix}\n",
    "dh2 = dlogits \\times (W3)^T \\\\\n",
    "dW3 = (h2)^T \\times dlogits \\\\\n",
    "db3 = \\text{sum of columns of dlogits}\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh2 = dlogits @ W3.T\n",
    "dW3 = h2.T @ dlogits\n",
    "db3 = dlogits.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h2 = tanh(h2\\_pre) \\quad \\Rightarrow \\quad dh2\\_pre = (1 - (h2)^2) \\cdot dh2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh2_pre = (1.0 - h2**2) * dh2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h2\\_pre = h1 \\times W2 + b2 \\quad \\Rightarrow \\quad\n",
    "\\begin{matrix}\n",
    "dh1 = dh2\\_pre \\times (W2)^T \\\\\n",
    "dW2 = (h1)^T \\times dh2\\_pre \\\\\n",
    "db2 = \\text{sum of columns of dh2\\_pre}\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh1 = dh2_pre @ W2.T\n",
    "dW2 = h1.T @ dh2_pre\n",
    "db2 = dh2_pre.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h1 = tanh(h1\\_pre) \\quad \\Rightarrow \\quad dh1\\_pre = (1 - (h1)^2) \\cdot dh1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh1_pre = (1.0 - h1**2) * dh1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h1\\_pre = X \\times W1 + b1 \\quad \\Rightarrow \\quad\n",
    "\\begin{matrix}\n",
    "dh1 = dh1\\_pre \\times (W1)^T \\\\\n",
    "dW1 = (X)^T \\times dh1\\_pre \\\\\n",
    "db1 = \\text{sum of columns of dh1\\_pre}\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX = dh1_pre @ W1.T\n",
    "dW1 = X.T @ dh1_pre\n",
    "db1 = dh1_pre.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a list that contains the gradient of the loss function with respect to the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = [dW1, db1, dW2, db2, dW3, db3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; A gradient is a vector that indicates the direction and rate of the steepest increase in a function's value. To minimize the loss function, we use **gradient descent**, an optimization algorithm that updates the model's parameters by moving in the opposite direction of the gradient.\n",
    "\n",
    "```{admonition} Gradient descent\n",
    "\n",
    "Gradient descent updates each parameter by subtracting a fraction of the gradient from its current value, scaled by a factor known as the learning rate. \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\cdot \\nabla L(\\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\theta$ represents the parameters of the neural network.\n",
    "- $\\eta$ is the learning rate.\n",
    "- $\\nabla L(\\theta)$ is the gradient of the loss function with respect to the parameters.\n",
    "```\n",
    "\n",
    "\n",
    "```{important}\n",
    "The learning rate determines the size of the steps we take towards the minimum. A smaller learning rate results in more precise but slower convergence, while a larger learning rate can speed up convergence but may risk overshooting the minimum.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1 \n",
    "for p, grad in zip(parameters, grads):\n",
    "    p.data += -lr * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "&ensp; &ensp; &ensp; &ensp; During **training**, a neural network iteratively performs a forward pass, calculates the loss, makes a backward pass, and updates its parameters. In this case, since the entire trainig dataset is processed in each training iteration, we can refer to each iteration as an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0/100     Loss: 15.9699\n",
      "Epoch: 10/100     Loss: 10.9262\n",
      "Epoch: 20/100     Loss: 8.3451\n",
      "Epoch: 30/100     Loss: 6.7141\n",
      "Epoch: 40/100     Loss: 5.6266\n",
      "Epoch: 50/100     Loss: 4.8790\n",
      "Epoch: 60/100     Loss: 4.3457\n",
      "Epoch: 70/100     Loss: 3.9500\n",
      "Epoch: 80/100     Loss: 3.6406\n",
      "Epoch: 90/100     Loss: 3.3900\n",
      "Epoch: 99/100     Loss: 3.2024\n"
     ]
    }
   ],
   "source": [
    "epochs = 100       # train iterations\n",
    "lr = 0.1           # learning rate\n",
    "\n",
    "# intialize neural network\n",
    "parameters = initialize_nn()\n",
    "W1, b1, W2, b2, W3, b3 = parameters\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # -------------------- forward pass --------------------\n",
    "\n",
    "    # hidden layer 1\n",
    "    h1_pre = X @ W1 + b1\n",
    "    h1 = torch.tanh(h1_pre)\n",
    "\n",
    "    # hidden layer 2\n",
    "    h2_pre = h1 @ W2 + b2\n",
    "    h2 = torch.tanh(h2_pre)\n",
    "\n",
    "    # output layer\n",
    "    logits = h2 @ W3 + b3\n",
    "\n",
    "    # softmax\n",
    "    counts = logits.exp()\n",
    "    counts_sum = counts.sum(1, keepdims=True)\n",
    "    counts_sum_inv = counts_sum**-1\n",
    "    probs = counts * counts_sum_inv\n",
    "\n",
    "\n",
    "    # -------------------- calculate loss --------------------\n",
    "\n",
    "    # average negative log liklihood\n",
    "    log_probs = probs.log()\n",
    "    loss = -log_probs[range(N), train_y].mean()\n",
    "\n",
    "    # print loss every 10 epochs\n",
    "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch: {epoch:2d}/{epochs}     Loss: {loss.item():.4f}\")\n",
    "    \n",
    "\n",
    "    # -------------------- backward pass --------------------\n",
    "\n",
    "    dlog_probs = torch.zeros_like(log_probs)\n",
    "    dlog_probs[range(N), train_y] = -1.0 / N\n",
    "\n",
    "    dprobs = (1.0 / probs) * dlog_probs\n",
    "    dcounts = counts_sum_inv * dprobs\n",
    "    dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "    dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "    dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "    dlogits = counts * dcounts\n",
    "    dh2 = dlogits @ W3.T\n",
    "    dW3 = h2.T @ dlogits\n",
    "    db3 = dlogits.sum(0)\n",
    "    dh2_pre = (1.0 - h2**2) * dh2\n",
    "    dh1 = dh2_pre @ W2.T\n",
    "    dW2 = h1.T @ dh2_pre\n",
    "    db2 = dh2_pre.sum(0)\n",
    "    dh1_pre = (1.0 - h1**2) * dh1\n",
    "    dX = dh1_pre @ W1.T\n",
    "    dW1 = X.T @ dh1_pre\n",
    "    db1 = dh1_pre.sum(0)\n",
    "\n",
    "    grads = [dW1, db1, dW2, db2, dW3, db3]\n",
    "\n",
    "\n",
    "    # -------------------- update parameters --------------------\n",
    "\n",
    "    for p, grad in zip(parameters, grads):\n",
    "        p.data += -lr * grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
